{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_sol.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5J2JSW8hZ8Ce3a6lpO85Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/AMAS_Project/blob/Aske/DQN_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0m2Uuntne1t"
      },
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
        "            n_actions):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions = self.fc3(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
        "            max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_min = eps_end\n",
        "        self.eps_dec = eps_dec\n",
        "        self.lr = lr\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.mem_size = max_mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mem_cntr = 0\n",
        "        self.iter_cntr = 0\n",
        "        self.replace_target = 100\n",
        "\n",
        "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
        "                                    fc1_dims=256, fc2_dims=256)\n",
        "        self.Q_next = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
        "                                    fc1_dims=256, fc2_dims=256)\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = terminal\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
        "            actions = self.Q_eval.forward(state)\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        \n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "        \n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        action_batch = self.action_memory[batch]\n",
        "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "\n",
        "        q_pred = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "        q_next = self.Q_next.forward(new_state_batch)\n",
        "        q_eval = self.Q_next.forward(new_state_batch)\n",
        "        q_next[terminal_batch] = 0.0\n",
        "\n",
        "        max_actions = T.argmax(q_eval,dim=1)\n",
        "\n",
        "        # q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
        "        q_target = reward_batch + self.gamma*q_next[batch_index, max_actions]\n",
        "\n",
        "        loss = self.Q_eval.loss(q_target, q_pred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "\n",
        "        self.iter_cntr += 1\n",
        "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
        "                       else self.eps_min\n",
        "\n",
        "        if self.iter_cntr % self.replace_target == 0:\n",
        "          self.Q_next.load_state_dict(self.Q_eval.state_dict())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8gXSzoToEO_",
        "outputId": "84e3e832-b12b-4432-8dfe-5492abcc1434"
      },
      "source": [
        "!pip3 install Box2d-py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 10.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Box2d-py\n",
            "Successfully installed Box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v90bhXLn2Yq",
        "outputId": "3a395383-5702-43f8-d3ac-562dea374de9"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "                 input_dims=[8], lr=0.0005)\n",
        "    scores, eps_history = [], []\n",
        "    n_games = 600\n",
        "    \n",
        "    for i in range(n_games):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "\n",
        "        avg_score = np.mean(scores[-100:])\n",
        "\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "                'average score %.2f' % avg_score,\n",
        "                'epsilon %.2f' % agent.epsilon)\n",
        "    x = [i+1 for i in range(n_games)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode  0 score -122.92 average score -122.92 epsilon 1.00\n",
            "episode  1 score 16.62 average score -53.15 epsilon 0.94\n",
            "episode  2 score -282.61 average score -129.64 epsilon 0.87\n",
            "episode  3 score -505.21 average score -223.53 epsilon 0.82\n",
            "episode  4 score -164.42 average score -211.71 epsilon 0.76\n",
            "episode  5 score -107.86 average score -194.40 epsilon 0.72\n",
            "episode  6 score -141.84 average score -186.89 epsilon 0.67\n",
            "episode  7 score -231.96 average score -192.52 epsilon 0.60\n",
            "episode  8 score -317.97 average score -206.46 epsilon 0.55\n",
            "episode  9 score -122.11 average score -198.03 epsilon 0.49\n",
            "episode  10 score -69.85 average score -186.38 epsilon 0.44\n",
            "episode  11 score -140.93 average score -182.59 epsilon 0.38\n",
            "episode  12 score -63.45 average score -173.42 epsilon 0.27\n",
            "episode  13 score -550.28 average score -200.34 epsilon 0.15\n",
            "episode  14 score -373.26 average score -211.87 epsilon 0.10\n",
            "episode  15 score -145.39 average score -207.72 epsilon 0.03\n",
            "episode  16 score -363.93 average score -216.90 epsilon 0.01\n",
            "episode  17 score -501.32 average score -232.71 epsilon 0.01\n",
            "episode  18 score -213.61 average score -231.70 epsilon 0.01\n",
            "episode  19 score -488.45 average score -244.54 epsilon 0.01\n",
            "episode  20 score -227.73 average score -243.74 epsilon 0.01\n",
            "episode  21 score -172.14 average score -240.48 epsilon 0.01\n",
            "episode  22 score 43.47 average score -228.14 epsilon 0.01\n",
            "episode  23 score -0.20 average score -218.64 epsilon 0.01\n",
            "episode  24 score -175.98 average score -216.93 epsilon 0.01\n",
            "episode  25 score -104.16 average score -212.60 epsilon 0.01\n",
            "episode  26 score -364.54 average score -218.22 epsilon 0.01\n",
            "episode  27 score -134.34 average score -215.23 epsilon 0.01\n",
            "episode  28 score -72.49 average score -210.31 epsilon 0.01\n",
            "episode  29 score -60.86 average score -205.32 epsilon 0.01\n",
            "episode  30 score -58.32 average score -200.58 epsilon 0.01\n",
            "episode  31 score -311.91 average score -204.06 epsilon 0.01\n",
            "episode  32 score -326.54 average score -207.77 epsilon 0.01\n",
            "episode  33 score -50.50 average score -203.15 epsilon 0.01\n",
            "episode  34 score -263.75 average score -204.88 epsilon 0.01\n",
            "episode  35 score -101.12 average score -202.00 epsilon 0.01\n",
            "episode  36 score -114.97 average score -199.64 epsilon 0.01\n",
            "episode  37 score -72.86 average score -196.31 epsilon 0.01\n",
            "episode  38 score -31.16 average score -192.07 epsilon 0.01\n",
            "episode  39 score -65.89 average score -188.92 epsilon 0.01\n",
            "episode  40 score -170.73 average score -188.47 epsilon 0.01\n",
            "episode  41 score -31.92 average score -184.75 epsilon 0.01\n",
            "episode  42 score -14.94 average score -180.80 epsilon 0.01\n",
            "episode  43 score -175.60 average score -180.68 epsilon 0.01\n",
            "episode  44 score -102.31 average score -178.94 epsilon 0.01\n",
            "episode  45 score -111.26 average score -177.47 epsilon 0.01\n",
            "episode  46 score -147.40 average score -176.83 epsilon 0.01\n",
            "episode  47 score -38.67 average score -173.95 epsilon 0.01\n",
            "episode  48 score -215.34 average score -174.79 epsilon 0.01\n",
            "episode  49 score -87.56 average score -173.05 epsilon 0.01\n",
            "episode  50 score -46.87 average score -170.58 epsilon 0.01\n",
            "episode  51 score -70.55 average score -168.65 epsilon 0.01\n",
            "episode  52 score -34.23 average score -166.12 epsilon 0.01\n",
            "episode  53 score -206.40 average score -166.86 epsilon 0.01\n",
            "episode  54 score -42.62 average score -164.60 epsilon 0.01\n",
            "episode  55 score -136.82 average score -164.11 epsilon 0.01\n",
            "episode  56 score -268.84 average score -165.94 epsilon 0.01\n",
            "episode  57 score -241.77 average score -167.25 epsilon 0.01\n",
            "episode  58 score -88.17 average score -165.91 epsilon 0.01\n",
            "episode  59 score -174.34 average score -166.05 epsilon 0.01\n",
            "episode  60 score -202.28 average score -166.64 epsilon 0.01\n",
            "episode  61 score -190.72 average score -167.03 epsilon 0.01\n",
            "episode  62 score -219.57 average score -167.87 epsilon 0.01\n",
            "episode  63 score -304.23 average score -170.00 epsilon 0.01\n",
            "episode  64 score -226.71 average score -170.87 epsilon 0.01\n",
            "episode  65 score -219.56 average score -171.61 epsilon 0.01\n",
            "episode  66 score -226.34 average score -172.43 epsilon 0.01\n",
            "episode  67 score -159.12 average score -172.23 epsilon 0.01\n",
            "episode  68 score -46.28 average score -170.40 epsilon 0.01\n",
            "episode  69 score -83.56 average score -169.16 epsilon 0.01\n",
            "episode  70 score -238.83 average score -170.14 epsilon 0.01\n",
            "episode  71 score -58.05 average score -168.59 epsilon 0.01\n",
            "episode  72 score -55.07 average score -167.03 epsilon 0.01\n",
            "episode  73 score -93.12 average score -166.03 epsilon 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkNQPtE-xU_O"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "# ja jeg har tyv stjålet dette\n",
        "# og nej jeg har ingen anse om hvad det gør men det virker!\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a8tfgt8oA6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "31a6e6c3-0a8f-4979-a9ac-245b468c237e"
      },
      "source": [
        "env = wrap_env(env = gym.make('LunarLander-v2'))\n",
        "observation = env.reset()\n",
        "action_space_size = env.action_space.n\n",
        "observation_space_size = env.observation_space.shape\n",
        "state_space_size = 8\n",
        "final_score = 0\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = agent.choose_action(observation)\n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "    final_score += reward    \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9404f572a4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maction_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobservation_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbZUi0gx3iA",
        "outputId": "8f03eb5f-5506-4d6a-ba54-d1a3ce97fd77"
      },
      "source": [
        "final_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-55.07237619753844"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51cItNTmzoJ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}