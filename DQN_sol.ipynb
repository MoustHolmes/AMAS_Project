{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_sol.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPATnB596uEQ65ebdqy5mEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/AMAS_Project/blob/Aske/DQN_sol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0m2Uuntne1t"
      },
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "class DeepQNetwork(nn.Module):\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, \n",
        "            n_actions):\n",
        "        super(DeepQNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        actions = self.fc3(x)\n",
        "\n",
        "        return actions\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, gamma, epsilon, lr, input_dims, batch_size, n_actions,\n",
        "            max_mem_size=100000, eps_end=0.05, eps_dec=5e-4):\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_min = eps_end\n",
        "        self.eps_dec = eps_dec\n",
        "        self.lr = lr\n",
        "        self.action_space = [i for i in range(n_actions)]\n",
        "        self.mem_size = max_mem_size\n",
        "        self.batch_size = batch_size\n",
        "        self.mem_cntr = 0\n",
        "        self.iter_cntr = 0\n",
        "        self.replace_target = 100\n",
        "\n",
        "        self.Q_eval = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
        "                                    fc1_dims=256, fc2_dims=256)\n",
        "        self.Q_next = DeepQNetwork(lr, n_actions=n_actions, input_dims=input_dims,\n",
        "                                    fc1_dims=256, fc2_dims=256)\n",
        "\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype=np.float32)\n",
        "        self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, terminal):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.reward_memory[index] = reward\n",
        "        self.action_memory[index] = action\n",
        "        self.terminal_memory[index] = terminal\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        if np.random.random() > self.epsilon:\n",
        "            state = T.tensor([observation]).to(self.Q_eval.device)\n",
        "            actions = self.Q_eval.forward(state)\n",
        "            action = T.argmax(actions).item()\n",
        "        else:\n",
        "            action = np.random.choice(self.action_space)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self):\n",
        "        if self.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        self.Q_eval.optimizer.zero_grad()\n",
        "        \n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        batch = np.random.choice(max_mem, self.batch_size, replace=False)\n",
        "        \n",
        "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
        "\n",
        "        state_batch = T.tensor(self.state_memory[batch]).to(self.Q_eval.device)\n",
        "        new_state_batch = T.tensor(self.new_state_memory[batch]).to(self.Q_eval.device)\n",
        "        action_batch = self.action_memory[batch]\n",
        "        reward_batch = T.tensor(self.reward_memory[batch]).to(self.Q_eval.device)\n",
        "        terminal_batch = T.tensor(self.terminal_memory[batch]).to(self.Q_eval.device)\n",
        "\n",
        "        q_pred = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "        q_next = self.Q_next.forward(new_state_batch)\n",
        "        q_eval = self.Q_eval.forward(new_state_batch)\n",
        "        q_next[terminal_batch] = 0.0\n",
        "\n",
        "        max_actions = T.argmax(q_eval,dim=1)\n",
        "\n",
        "        # q_target = reward_batch + self.gamma*T.max(q_next,dim=1)[0]\n",
        "        q_target = reward_batch + self.gamma*q_next[batch_index, max_actions]\n",
        "\n",
        "        loss = self.Q_eval.loss(q_target, q_pred).to(self.Q_eval.device)\n",
        "        loss.backward()\n",
        "        self.Q_eval.optimizer.step()\n",
        "\n",
        "        self.iter_cntr += 1\n",
        "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min \\\n",
        "                       else self.eps_min\n",
        "\n",
        "        if self.iter_cntr % self.replace_target == 0:\n",
        "          self.Q_next.load_state_dict(self.Q_eval.state_dict())"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8gXSzoToEO_",
        "outputId": "84e3e832-b12b-4432-8dfe-5492abcc1434"
      },
      "source": [
        "!pip3 install Box2d-py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 10.7MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 14.7MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 9.7MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 5.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 5.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 5.8MB/s \n",
            "\u001b[?25hInstalling collected packages: Box2d-py\n",
            "Successfully installed Box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5v90bhXLn2Yq",
        "outputId": "10fa85e5-4773-44ec-e23d-f0e5990deec5"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    agent = Agent(gamma=0.99, epsilon=1.0, batch_size=64, n_actions=4, eps_end=0.01,\n",
        "                 input_dims=[8], lr=0.0005)\n",
        "    scores, eps_history = [], []\n",
        "    n_games = 600\n",
        "    \n",
        "    for i in range(n_games):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.store_transition(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        eps_history.append(agent.epsilon)\n",
        "\n",
        "        avg_score = np.mean(scores[-100:])\n",
        "\n",
        "        print('episode ', i, 'score %.2f' % score,\n",
        "                'average score %.2f' % avg_score,\n",
        "                'epsilon %.2f' % agent.epsilon)\n",
        "    x = [i+1 for i in range(n_games)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode  0 score -102.17 average score -102.17 epsilon 1.00\n",
            "episode  1 score -412.06 average score -257.12 epsilon 0.95\n",
            "episode  2 score -236.66 average score -250.30 epsilon 0.90\n",
            "episode  3 score -130.14 average score -220.26 epsilon 0.86\n",
            "episode  4 score -124.21 average score -201.05 epsilon 0.82\n",
            "episode  5 score -183.00 average score -198.04 epsilon 0.74\n",
            "episode  6 score 41.20 average score -163.86 epsilon 0.70\n",
            "episode  7 score -388.36 average score -191.92 epsilon 0.62\n",
            "episode  8 score -23.23 average score -173.18 epsilon 0.56\n",
            "episode  9 score -144.11 average score -170.27 epsilon 0.49\n",
            "episode  10 score -120.71 average score -165.77 epsilon 0.45\n",
            "episode  11 score -107.78 average score -160.94 epsilon 0.40\n",
            "episode  12 score -186.48 average score -162.90 epsilon 0.35\n",
            "episode  13 score -101.83 average score -158.54 epsilon 0.31\n",
            "episode  14 score -179.06 average score -159.91 epsilon 0.24\n",
            "episode  15 score -387.38 average score -174.12 epsilon 0.19\n",
            "episode  16 score -172.43 average score -174.03 epsilon 0.14\n",
            "episode  17 score -138.58 average score -172.06 epsilon 0.07\n",
            "episode  18 score -291.39 average score -178.34 epsilon 0.01\n",
            "episode  19 score -241.08 average score -181.47 epsilon 0.01\n",
            "episode  20 score -163.37 average score -180.61 epsilon 0.01\n",
            "episode  21 score -89.10 average score -176.45 epsilon 0.01\n",
            "episode  22 score -132.11 average score -174.52 epsilon 0.01\n",
            "episode  23 score -287.85 average score -179.25 epsilon 0.01\n",
            "episode  24 score -568.72 average score -194.83 epsilon 0.01\n",
            "episode  25 score -454.78 average score -204.82 epsilon 0.01\n",
            "episode  26 score -222.66 average score -205.48 epsilon 0.01\n",
            "episode  27 score -140.28 average score -203.16 epsilon 0.01\n",
            "episode  28 score -578.05 average score -216.08 epsilon 0.01\n",
            "episode  29 score -294.23 average score -218.69 epsilon 0.01\n",
            "episode  30 score -219.30 average score -218.71 epsilon 0.01\n",
            "episode  31 score -654.37 average score -232.32 epsilon 0.01\n",
            "episode  32 score -418.39 average score -237.96 epsilon 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkNQPtE-xU_O"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "# ja jeg har tyv stjålet dette\n",
        "# og nej jeg har ingen anse om hvad det gør men det virker!\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a8tfgt8oA6h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "31a6e6c3-0a8f-4979-a9ac-245b468c237e"
      },
      "source": [
        "env = wrap_env(env = gym.make('LunarLander-v2'))\n",
        "observation = env.reset()\n",
        "action_space_size = env.action_space.n\n",
        "observation_space_size = env.observation_space.shape\n",
        "state_space_size = 8\n",
        "final_score = 0\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = agent.choose_action(observation)\n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "    final_score += reward    \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-9404f572a4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maction_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mobservation_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCbZUi0gx3iA",
        "outputId": "8f03eb5f-5506-4d6a-ba54-d1a3ce97fd77"
      },
      "source": [
        "final_score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-55.07237619753844"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51cItNTmzoJ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}