{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RFL_AMAS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/AMAS_Project/blob/Martin/AMAS_Gradient_policy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewx3FEHcJLjy"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhGyv5e4UWjb",
        "outputId": "0f891d36-b333-4313-8255-c10080fb4d1a"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |▊                               | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 9.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 8.4MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.2MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 122kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 174kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 194kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 225kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 245kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 296kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 317kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 337kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 348kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 368kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 389kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 399kB 6.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 409kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 440kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 6.1MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQskruJraoYE"
      },
      "source": [
        "## Gradient policy v0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kP9C84ynLAiV"
      },
      "source": [
        "### Imports\n",
        "\n",
        "```\n",
        "# Dette formateres som kode\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNxky3dvLKxD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Categorical\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kszUcF0-Ljzj"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRHquDZZLQ6E"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(1); torch.manual_seed(1);"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t2JagI-LcuV"
      },
      "source": [
        "###  Policy gradients\n",
        "A policy gradient attempts to train an agent without explicitly mapping the value for every state-action pair in an environment by taking small steps and updating the policy based on the reward associated with that step. The agent can receive a reward immediately for an action or the agent can receive the reward at a later time such as the end of the episode.  We’ll designate the policy function our agent is trying to learn as $\\pi_\\theta(a,s)$, where $\\theta$ is the parameter vector, $s$ is a particular state, and $a$ is an action.\n",
        "\n",
        "We'll apply a technique called Monte-Carlo Policy Gradient which means we will have the agent run through an entire episode and then update our policy based on the rewards obtained."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58WYhhggL17U"
      },
      "source": [
        "### Model construction "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx1E84KbL6Nu"
      },
      "source": [
        "#### Create Neural Network Model\n",
        "We will use a simple feed forward neural network with one hidden layer of 128 neurons and a dropout of 0.6. We'll use Adam as our optimizer and a learning rate of 0.01. Using dropout will significantly improve the performance of our policy. I encourage you to compare results with and without dropout and experiment with other hyper-parameter values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfEdKDiL1SF"
      },
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.state_space = env.observation_space.shape[0]\n",
        "        self.action_space = env.action_space.n\n",
        "        \n",
        "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
        "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
        "        \n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Episode policy and reward history \n",
        "        self.policy_history = Variable(torch.Tensor()) \n",
        "        self.reward_episode = []\n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "\n",
        "    def forward(self, x):    \n",
        "        model = torch.nn.Sequential(\n",
        "            self.l1,\n",
        "            nn.Dropout(p=0.6),\n",
        "            nn.ReLU(),\n",
        "            self.l2,\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "        return model(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QODHDjjoMKtG"
      },
      "source": [
        "policy = Policy()\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkeL4QwVMOKc"
      },
      "source": [
        "#### Select Action\n",
        "The select_action function chooses an action based on our policy probability distribution using the PyTorch distributions package. Our policy returns a probability for each possible action in our action space (move left or move right) as an array of length two such as [0.7, 0.3]. We then choose an action based on these probabilities, record our history, and return our action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LPm0SjPMj5i"
      },
      "source": [
        "def select_action(state):\n",
        "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
        "    state = torch.from_numpy(state).type(torch.FloatTensor)\n",
        "    state = policy(Variable(state))\n",
        "    c = Categorical(state)\n",
        "    action = c.sample()\n",
        "    \n",
        "    # Add log probability of our chosen action to our history    \n",
        "    if policy.policy_history.dim() != 0:\n",
        "        policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action)])\n",
        "    else:\n",
        "        policy.policy_history = (c.log_prob(action))\n",
        "    return action"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UToI34bqMn3f"
      },
      "source": [
        "#### Reward $v_t$\n",
        "We update our policy by taking a sample of the action value function $Q^{\\pi_\\theta} (s_t,a_t)$ by playing through episodes of the game. $Q^{\\pi_\\theta} (s_t,a_t)$ is defined as the expected return by taking action $a$ in state $s$ following policy $\\pi$.\n",
        "\n",
        "We know that for every step the simulation continues we receive a reward of 1. We can use this to calculate the policy gradient at each time step, where $r$ is the reward for a particular state-action pair. Rather than using the instantaneous reward, $r$, we instead use a long term reward $ v_{t} $ where $v_t$ is the discounted sum of all future rewards for the length of the episode. In this way, the longer the episode runs into the future, the greater the reward for a particular state-action pair in the present. $v_{t}$ is then,\n",
        "\n",
        "$$ v_{t} = \\sum_{k=0}^{N} \\gamma^{k}r_{t+k} $$\n",
        "where $\\gamma$ is the discount factor (0.99). For example, if an episode lasts 5 steps, the reward for each step will be [4.90, 3.94, 2.97, 1.99, 1]. Next we scale our reward vector by substracting the mean from each element and scaling to unit variance by dividing by the standard deviation. This practice is common for machine learning applications and the same operation as Scikit Learn's StandardScaler. It also has the effect of compensating for future uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFmtW_tsNR8E"
      },
      "source": [
        "### Update Policy\n",
        "After each episode we apply Monte-Carlo Policy Gradient to improve our policy according to the equation:\n",
        "\n",
        "$$\\Delta\\theta_t = \\alpha\\nabla_\\theta \\, \\log \\pi_\\theta (s_t,a_t)v_t  $$\n",
        "We will then feed our policy history multiplied by our rewards to our optimizer and update the weights of our neural network using stochastic gradent ascent. This should increase the likelihood of actions that got our agent a larger reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awzfDvV5Mj6R"
      },
      "source": [
        "def update_policy():\n",
        "    R = 0\n",
        "    rewards = []\n",
        "    \n",
        "    # Discount future rewards back to the present using gamma\n",
        "    for r in policy.reward_episode[::-1]:\n",
        "        R = r + policy.gamma * R\n",
        "        rewards.insert(0,R)\n",
        "        \n",
        "    # Scale rewards\n",
        "    rewards = torch.FloatTensor(rewards)\n",
        "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
        "    \n",
        "    # Calculate loss\n",
        "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
        "    \n",
        "    # Update network weights\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    #Save and intialize episode history counters\n",
        "    policy.loss_history.append(loss.data[0])\n",
        "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
        "    policy.policy_history = Variable(torch.Tensor())\n",
        "    policy.reward_episode= []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nN6QPzEFNa-i"
      },
      "source": [
        "#### Training \n",
        "This is our main policy training loop. For each step in a training episode, we choose an action, take a step through the environment, and record the resulting new state and reward. We call update_policy() at the end of each episode to feed the episode history to our neural network and improve our policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smMMuj7DNf8g"
      },
      "source": [
        "def main(episodes):\n",
        "    running_reward = 10\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset() # Reset environment and record the starting state\n",
        "        done = False       \n",
        "    \n",
        "        for time in range(1000):\n",
        "            action = select_action(state)\n",
        "            # Step through environment using chosen action\n",
        "            state, reward, done, _ = env.step(action.data[0])\n",
        "\n",
        "            # Save reward\n",
        "            policy.reward_episode.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Used to determine when the environment is solved.\n",
        "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
        "\n",
        "        update_policy()\n",
        "\n",
        "        if episode % 50 == 0:\n",
        "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(episode, time, running_reward))\n",
        "\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and the last episode runs to {} time steps!\".format(running_reward, time))\n",
        "            break"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpQtUDNHNhW4"
      },
      "source": [
        "### Run model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MShZXW8QNjra"
      },
      "source": [
        "episodes = 1000\n",
        "main(episodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13RX2AzmavEq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K0YaWRVawBQ"
      },
      "source": [
        "## Gradient policy V0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h9tc2wZJSW0"
      },
      "source": [
        "### installs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv68xi8Ifkxr",
        "outputId": "11b8fa9a-d08c-4400-d8fe-fbce0c818019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgZ4rf88Iz6D"
      },
      "source": [
        "###Video render function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HY5cEfWjKDcp",
        "outputId": "03016c4e-e825-47ae-fa76-d4239f495290"
      },
      "source": [
        "import gym\n",
        "\n",
        "# import wandb\n",
        "\n",
        "# # 1. Start a new run\n",
        "# wandb.init(project='RFL-AMAS', entity='maskel')\n",
        "\n",
        "# # 2. Save model inputs and hyperparameters\n",
        "# config = wandb.config\n",
        "# config.learning_rate = 0.01\n",
        "\n",
        "# # 3. Log gradients and model parameters\n",
        "# wandb.watch(model)\n",
        "# for batch_idx, (data, target) in enumerate(train_loader):\n",
        "#   ...\n",
        "#   if batch_idx % args.log_interval == 0:\n",
        "#     # 4. Log metrics to visualize performance\n",
        "#     wandb.log({\"loss\": loss})\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f67d67191d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8TbGo9YX1aX"
      },
      "source": [
        "# ja jeg har tyv stjålet dette\n",
        "# og nej jeg har ingen anse om hvad det gør men det virker!\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "u3ahv-SeJF8Z",
        "outputId": "dab401f9-5652-4ba0-c76f-0436738f9386"
      },
      "source": [
        "env = wrap_env(env = gym.make('LunarLander-v2'))\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = env.action_space.sample() \n",
        "         \n",
        "    observation, reward, done, info = env.step(action) \n",
        "   \n",
        "        \n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                loop controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD8clYJnbJCc"
      },
      "source": [
        "### Gradient policy algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaP3aEZv4LC6"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import argparse\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--env', help='CartPole or LunarLander OpenAI gym environment', type=str)\n",
        "parser.add_argument('--use_cuda', help='Use if you want to use CUDA', action='store_true')\n",
        "\n",
        "\n",
        "class Params:\n",
        "    NUM_EPOCHS = 1000\n",
        "    ALPHA = 5e-3        # learning rate\n",
        "    BATCH_SIZE = 20     # how many episodes we want to pack into an epoch\n",
        "    GAMMA = 0.99        # discount rate\n",
        "    HIDDEN_SIZE = 64    # number of hidden nodes we have in our dnn\n",
        "    BETA = 0.1          # the entropy bonus multiplier\n",
        "\n",
        "\n",
        "# Q-table is replaced by a neural network\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, observation_space_size: int, action_space_size: int, hidden_size: int):\n",
        "        super(Agent, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features=observation_space_size, out_features=hidden_size, bias=True),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=hidden_size, bias=True),\n",
        "            nn.PReLU(),\n",
        "            nn.Linear(in_features=hidden_size, out_features=action_space_size, bias=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = normalize(x, dim=1)\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PolicyGradient:\n",
        "    def __init__(self, problem, use_cuda: bool = False):\n",
        "\n",
        "        self.NUM_EPOCHS = Params.NUM_EPOCHS\n",
        "        self.ALPHA = Params.ALPHA\n",
        "        self.BATCH_SIZE = Params.BATCH_SIZE\n",
        "        self.GAMMA = Params.GAMMA\n",
        "        self.HIDDEN_SIZE = Params.HIDDEN_SIZE\n",
        "        self.BETA = Params.BETA\n",
        "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
        "\n",
        "        # instantiate the tensorboard writer\n",
        "        self.writer = SummaryWriter(comment=f'_PG_CP_Gamma={self.GAMMA},'\n",
        "                                            f'LR={self.ALPHA},'\n",
        "                                            f'BS={self.BATCH_SIZE},'\n",
        "                                            f'NH={self.HIDDEN_SIZE},'\n",
        "                                            f'BETA={self.BETA}')\n",
        "\n",
        "        # create the environment\n",
        "        self.env = problem\n",
        "\n",
        "        # the agent driven by a neural network architecture\n",
        "        self.agent = Agent(observation_space_size=self.env.observation_space.shape[0],\n",
        "                           action_space_size=self.env.action_space.n,\n",
        "                           hidden_size=self.HIDDEN_SIZE).to(self.DEVICE)\n",
        "\n",
        "        self.adam = optim.Adam(params=self.agent.parameters(), lr=self.ALPHA)\n",
        "\n",
        "        self.total_rewards = deque([], maxlen=100)\n",
        "\n",
        "        # flag to figure out if we have render a single episode current epoch\n",
        "        self.finished_rendering_this_epoch = False\n",
        "\n",
        "    def solve_environment(self):\n",
        "        \"\"\"\n",
        "            The main interface for the Policy Gradient solver\n",
        "        \"\"\"\n",
        "        # init the episode and the epoch\n",
        "        episode = 0\n",
        "        epoch = 0\n",
        "\n",
        "        # init the epoch arrays\n",
        "        # used for entropy calculation\n",
        "        epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
        "        epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
        "\n",
        "        while True:\n",
        "\n",
        "            # play an episode of the environment\n",
        "            (episode_weighted_log_prob_trajectory,\n",
        "             episode_logits,\n",
        "             sum_of_episode_rewards,\n",
        "             episode) = self.play_episode(episode=episode)\n",
        "\n",
        "            # after each episode append the sum of total rewards to the deque\n",
        "            self.total_rewards.append(sum_of_episode_rewards)\n",
        "\n",
        "            # append the weighted log-probabilities of actions\n",
        "            epoch_weighted_log_probs = torch.cat((epoch_weighted_log_probs, episode_weighted_log_prob_trajectory),\n",
        "                                                 dim=0)\n",
        "\n",
        "            # append the logits - needed for the entropy bonus calculation\n",
        "            epoch_logits = torch.cat((epoch_logits, episode_logits), dim=0)\n",
        "\n",
        "            # if the epoch is over - we have epoch trajectories to perform the policy gradient\n",
        "            if episode >= self.BATCH_SIZE:\n",
        "\n",
        "                # reset the rendering flag\n",
        "                self.finished_rendering_this_epoch = False\n",
        "\n",
        "                # reset the episode count\n",
        "                episode = 0\n",
        "\n",
        "                # increment the epoch\n",
        "                epoch += 1\n",
        "\n",
        "                # calculate the loss\n",
        "                loss, entropy = self.calculate_loss(epoch_logits=epoch_logits,\n",
        "                                                    weighted_log_probs=epoch_weighted_log_probs)\n",
        "\n",
        "                # zero the gradient\n",
        "                self.adam.zero_grad()\n",
        "\n",
        "                # backprop\n",
        "                loss.backward()\n",
        "\n",
        "                # update the parameters\n",
        "                self.adam.step()\n",
        "\n",
        "                # feedback\n",
        "                print(\"\\r\", f\"Epoch: {epoch}, Avg Return per Epoch: {np.mean(self.total_rewards):.3f}\",\n",
        "                      end=\"\")\n",
        "                     # ,flush=True)\n",
        "\n",
        "                self.writer.add_scalar(tag='Average Return over 100 episodes',\n",
        "                                       scalar_value=np.mean(self.total_rewards),\n",
        "                                       global_step=epoch)\n",
        "\n",
        "                self.writer.add_scalar(tag='Entropy',\n",
        "                                       scalar_value=entropy,\n",
        "                                       global_step=epoch)\n",
        "\n",
        "                # reset the epoch arrays\n",
        "                # used for entropy calculation\n",
        "                epoch_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
        "                epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
        "\n",
        "                # check if solved\n",
        "                if np.mean(self.total_rewards) > 200:\n",
        "                    print('\\nSolved!')\n",
        "                    break\n",
        "\n",
        "        # close the environment\n",
        "        self.env.close()\n",
        "\n",
        "        # close the writer\n",
        "        self.writer.close()\n",
        "\n",
        "    def play_episode(self, episode: int):\n",
        "        \"\"\"\n",
        "            Plays an episode of the environment.\n",
        "            episode: the episode counter\n",
        "            Returns:\n",
        "                sum_weighted_log_probs: the sum of the log-prob of an action multiplied by the reward-to-go from that state\n",
        "                episode_logits: the logits of every step of the episode - needed to compute entropy for entropy bonus\n",
        "                finished_rendering_this_epoch: pass-through rendering flag\n",
        "                sum_of_rewards: sum of the rewards for the episode - needed for the average over 200 episode statistic\n",
        "        \"\"\"\n",
        "        # reset the environment to a random initial state every epoch\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # initialize the episode arrays\n",
        "        episode_actions = torch.empty(size=(0,), dtype=torch.long, device=self.DEVICE)\n",
        "        episode_logits = torch.empty(size=(0, self.env.action_space.n), device=self.DEVICE)\n",
        "        average_rewards = np.empty(shape=(0,), dtype=np.float)\n",
        "        episode_rewards = np.empty(shape=(0,), dtype=np.float)\n",
        "\n",
        "        # episode loop\n",
        "        while True:\n",
        "\n",
        "            # render the environment for the first episode in the epoch\n",
        "            if not self.finished_rendering_this_epoch:\n",
        "                self.env.render()\n",
        "\n",
        "            # get the action logits from the agent - (preferences)\n",
        "            action_logits = self.agent(torch.tensor(state).float().unsqueeze(dim=0).to(self.DEVICE))\n",
        "\n",
        "            # append the logits to the episode logits list\n",
        "            episode_logits = torch.cat((episode_logits, action_logits), dim=0)\n",
        "\n",
        "            # sample an action according to the action distribution\n",
        "            action = Categorical(logits=action_logits).sample()\n",
        "\n",
        "            # append the action to the episode action list to obtain the trajectory\n",
        "            # we need to store the actions and logits so we could calculate the gradient of the performance\n",
        "            episode_actions = torch.cat((episode_actions, action), dim=0)\n",
        "\n",
        "            # take the chosen action, observe the reward and the next state\n",
        "            state, reward, done, _ = self.env.step(action=action.cpu().item())\n",
        "\n",
        "            # append the reward to the rewards pool that we collect during the episode\n",
        "            # we need the rewards so we can calculate the weights for the policy gradient\n",
        "            # and the baseline of average\n",
        "            episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
        "\n",
        "            # here the average reward is state specific\n",
        "            average_rewards = np.concatenate((average_rewards,\n",
        "                                              np.expand_dims(np.mean(episode_rewards), axis=0)),\n",
        "                                             axis=0)\n",
        "\n",
        "            # the episode is over\n",
        "            if done:\n",
        "\n",
        "                # increment the episode\n",
        "                episode += 1\n",
        "\n",
        "                # turn the rewards we accumulated during the episode into the rewards-to-go:\n",
        "                # earlier actions are responsible for more rewards than the later taken actions\n",
        "                discounted_rewards_to_go = PolicyGradient.get_discounted_rewards(rewards=episode_rewards,\n",
        "                                                                                 gamma=self.GAMMA)\n",
        "                discounted_rewards_to_go -= average_rewards  # baseline - state specific average\n",
        "\n",
        "                # # calculate the sum of the rewards for the running average metric\n",
        "                sum_of_rewards = np.sum(episode_rewards)\n",
        "\n",
        "                # set the mask for the actions taken in the episode\n",
        "                mask = one_hot(episode_actions, num_classes=self.env.action_space.n)\n",
        "\n",
        "                # calculate the log-probabilities of the taken actions\n",
        "                # mask is needed to filter out log-probabilities of not related logits\n",
        "                episode_log_probs = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
        "\n",
        "                # weight the episode log-probabilities by the rewards-to-go\n",
        "                episode_weighted_log_probs = episode_log_probs * \\\n",
        "                    torch.tensor(discounted_rewards_to_go).float().to(self.DEVICE)\n",
        "\n",
        "                # calculate the sum over trajectory of the weighted log-probabilities\n",
        "                sum_weighted_log_probs = torch.sum(episode_weighted_log_probs).unsqueeze(dim=0)\n",
        "\n",
        "                # won't render again this epoch\n",
        "                self.finished_rendering_this_epoch = True\n",
        "\n",
        "                return sum_weighted_log_probs, episode_logits, sum_of_rewards, episode\n",
        "\n",
        "    def calculate_loss(self, epoch_logits: torch.Tensor, weighted_log_probs: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
        "        \"\"\"\n",
        "            Calculates the policy \"loss\" and the entropy bonus\n",
        "            Args:\n",
        "                epoch_logits: logits of the policy network we have collected over the epoch\n",
        "                weighted_log_probs: loP * W of the actions taken\n",
        "            Returns:\n",
        "                policy loss + the entropy bonus\n",
        "                entropy: needed for logging\n",
        "        \"\"\"\n",
        "        policy_loss = -1 * torch.mean(weighted_log_probs)\n",
        "\n",
        "        # add the entropy bonus\n",
        "        p = softmax(epoch_logits, dim=1)\n",
        "        log_p = log_softmax(epoch_logits, dim=1)\n",
        "        entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
        "        entropy_bonus = -1 * self.BETA * entropy\n",
        "\n",
        "        return policy_loss + entropy_bonus, entropy\n",
        "\n",
        "    @staticmethod\n",
        "    def get_discounted_rewards(rewards: np.array, gamma: float) -> np.array:\n",
        "        \"\"\"\n",
        "            Calculates the sequence of discounted rewards-to-go.\n",
        "            Args:\n",
        "                rewards: the sequence of observed rewards\n",
        "                gamma: the discount factor\n",
        "            Returns:\n",
        "                discounted_rewards: the sequence of the rewards-to-go\n",
        "        \"\"\"\n",
        "        discounted_rewards = np.empty_like(rewards, dtype=np.float)\n",
        "        for i in range(rewards.shape[0]):\n",
        "            gammas = np.full(shape=(rewards[i:].shape[0]), fill_value=gamma)\n",
        "            discounted_gammas = np.power(gammas, np.arange(rewards[i:].shape[0]))\n",
        "            discounted_reward = np.sum(rewards[i:] * discounted_gammas)\n",
        "            discounted_rewards[i] = discounted_reward\n",
        "        return discounted_rewards"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuR4tV_VUwe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a2b9a9-1b9a-4a95-baf2-af93da9cb969"
      },
      "source": [
        "policy_gradient = PolicyGradient(problem=gym.make('LunarLander-v2'),use_cuda=True)\n",
        "policy_gradient.solve_environment()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch: 70, Avg Return per Epoch: -69.776"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}