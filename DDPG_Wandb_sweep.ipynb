{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_Wandb_sweep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/AMAS_Project/blob/main/DDPG_Wandb_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oQTNVeq63Cm"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgdLryIc12tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67b6d043-c6cc-4cda-ee22-073aefb99c9b"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip install wandb"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.7/dist-packages (2.3.8)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.23)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.14)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.1.2)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.6)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL8C74KZ9GqA",
        "outputId": "5f1d4ebc-bae2-435c-800c-689eacaee64d"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoustholmes\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXeDEL7e7D8o"
      },
      "source": [
        "## Define the Q Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpZru0S1fOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5c8fc7d-6779-47d8-c379-6777ed53112d"
      },
      "source": [
        "%%writefile DDPG_Agent.py\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                 chkpt_dir='/content/gdrive/My Drive'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.name = name\n",
        "        model_save_name = name + '_2.pt'\n",
        "        # self.path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        self.checkpoint_dir = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "        # self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "\n",
        "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
        "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
        "\n",
        "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
        "        \n",
        "        self.q = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        self.fc1.weight.data.uniform_(-f1, f1)\n",
        "        self.fc1.bias.data.uniform_(-f1, f1)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        self.fc2.weight.data.uniform_(-f2, f2)\n",
        "        self.fc2.bias.data.uniform_(-f2, f2)\n",
        "\n",
        "        f3 = 0.003\n",
        "        self.q.weight.data.uniform_(-f3, f3)\n",
        "        self.q.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "        f4 = 1./np.sqrt(self.action_value.weight.data.size()[0])\n",
        "        \n",
        "        self.action_value.weight.data.uniform_(-f4, f4)\n",
        "        self.action_value.bias.data.uniform_(-f4, f4)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta,\n",
        "                                    weight_decay=0.01)\n",
        "        \n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_value = self.fc1(state)\n",
        "        state_value = self.bn1(state_value)\n",
        "        state_value = F.relu(state_value)\n",
        "        state_value = self.fc2(state_value)\n",
        "        state_value = self.bn2(state_value)\n",
        "        #state_value = F.relu(state_value)\n",
        "        #action_value = F.relu(self.action_value(action))\n",
        "        action_value = self.action_value(action)\n",
        "        state_action_value = F.relu(T.add(state_value, action_value))\n",
        "        #state_action_value = T.add(state_value, action_value)\n",
        "        state_action_value = self.q(state_action_value)\n",
        "\n",
        "        return state_action_value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        # T.save(self.state_dict(), self.path)\n",
        "        # T.save(self.state_dict(), self.checkpoint_file)\n",
        "        T.save(self.state_dict(), self.checkpoint_dir)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_dir))\n",
        "\n",
        "    def save_best(self):\n",
        "        print('... saving best checkpoint ...')\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
        "        T.save(self.state_dict(), checkpoint_file)\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                 chkpt_dir='/content/gdrive/My Drive'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.name = name\n",
        "        model_save_name = name + '_2.pt'\n",
        "        # self.path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        self.checkpoint_dir = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "        # self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "\n",
        "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "\n",
        "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
        "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
        "\n",
        "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        self.fc2.weight.data.uniform_(-f2, f2)\n",
        "        self.fc2.bias.data.uniform_(-f2, f2)\n",
        "\n",
        "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        self.fc1.weight.data.uniform_(-f1, f1)\n",
        "        self.fc1.bias.data.uniform_(-f1, f1)\n",
        "\n",
        "        f3 = 0.003\n",
        "        self.mu.weight.data.uniform_(-f3, f3)\n",
        "        self.mu.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = T.tanh(self.mu(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        # T.save(self.state_dict(), self.checkpoint_file)\n",
        "        T.save(self.state_dict(), self.checkpoint_dir)#changed file to dir \n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_dir))#changed file to dir\n",
        "\n",
        "    def save_best(self):\n",
        "        print('... saving best checkpoint ...')\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
        "        T.save(self.state_dict(), checkpoint_file)\n",
        "\n",
        "\n",
        "class OUActionNoise():\n",
        "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, input_dims, n_actions,\n",
        "                 alpha, beta, tau, gamma=0.99,\n",
        "                 output_bounds=1,\n",
        "                 fc1_dims=400, fc2_dims=300,\n",
        "                 noise_sigma = 0.15,\n",
        "                 batch_size=256,\n",
        "                 max_size=1000000):\n",
        "      #output bounds are my addition \n",
        "        self.output_bounds = output_bounds\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "        #we have  to scale the noise to match the outpu bounds\n",
        "\n",
        "        var_names = str(alpha) +'_'+ str(beta) +'_'+ str(gamma) +'_' +str(fc1_dims) +'_'+ str(fc2_dims) +'_'+ str(noise_sigma)\n",
        "\n",
        "        self.noise = OUActionNoise(mu=np.zeros(n_actions), sigma = noise_sigma *output_bounds)\n",
        "\n",
        "        self.actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions=n_actions, name='actor' +var_names)\n",
        "        self.critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions=n_actions, name='critic' +var_names)\n",
        "\n",
        "        self.target_actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions = n_actions, name = 'target_actor' +var_names)\n",
        "\n",
        "        self.target_critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions = n_actions, name = 'target_critic' +var_names)\n",
        "\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        self.actor.eval()\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state).to(self.actor.device)\n",
        "        mu_prime = mu + T.tensor(self.noise(), \n",
        "                                    dtype=T.float).to(self.actor.device)\n",
        "        self.actor.train()\n",
        "\n",
        "        return mu_prime.cpu().detach().numpy()[0]*self.output_bounds\n",
        "\n",
        "    def choose_action_no_noise(self, observation):#my addition\n",
        "        self.actor.eval()\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state).to(self.actor.device)\n",
        "        return mu.cpu().detach().numpy()[0]*self.output_bounds\n",
        "\n",
        "    def choose_action_no_noise_batch(self, observation):#my addition\n",
        "        self.actor.eval()\n",
        "        state = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state)#.to(self.actor.device)\n",
        "        return mu.cpu().detach().numpy()*self.output_bounds\n",
        "\n",
        "    def remember(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.actor.save_checkpoint()\n",
        "        self.target_actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "        self.target_critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.actor.load_checkpoint()\n",
        "        self.target_actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "        self.target_critic.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, states_, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = T.tensor(states, dtype=T.float).to(self.actor.device)\n",
        "        states_ = T.tensor(states_, dtype=T.float).to(self.actor.device)\n",
        "        actions = T.tensor(actions, dtype=T.float).to(self.actor.device)\n",
        "        rewards = T.tensor(rewards, dtype=T.float).to(self.actor.device)\n",
        "        done = T.tensor(done).to(self.actor.device)\n",
        "\n",
        "        target_actions = self.target_actor.forward(states_)\n",
        "        critic_value_ = self.target_critic.forward(states_, target_actions)\n",
        "        critic_value = self.critic.forward(states, actions)\n",
        "\n",
        "        critic_value_[done] = 0.0\n",
        "        critic_value_ = critic_value_.view(-1)\n",
        "\n",
        "        target = rewards + self.gamma*critic_value_\n",
        "        target = target.view(self.batch_size, 1)\n",
        "\n",
        "        self.critic.optimizer.zero_grad()\n",
        "        critic_loss = F.mse_loss(target, critic_value)\n",
        "        critic_loss.backward()\n",
        "        self.critic.optimizer.step()\n",
        "\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        actor_loss = -self.critic.forward(states, self.actor.forward(states))\n",
        "        actor_loss = T.mean(actor_loss)\n",
        "        actor_loss.backward()\n",
        "        self.actor.optimizer.step()\n",
        "\n",
        "        self.update_network_parameters()\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        actor_params = self.actor.named_parameters()\n",
        "        critic_params = self.critic.named_parameters()\n",
        "        target_actor_params = self.target_actor.named_parameters()\n",
        "        target_critic_params = self.target_critic.named_parameters()\n",
        "\n",
        "        critic_state_dict = dict(critic_params)\n",
        "        actor_state_dict = dict(actor_params)\n",
        "        target_critic_state_dict = dict(target_critic_params)\n",
        "        target_actor_state_dict = dict(target_actor_params)\n",
        "\n",
        "        for name in critic_state_dict:\n",
        "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
        "                                (1-tau)*target_critic_state_dict[name].clone()\n",
        "\n",
        "        for name in actor_state_dict:\n",
        "             actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
        "                                 (1-tau)*target_actor_state_dict[name].clone()\n",
        "\n",
        "        # self.target_critic.load_state_dict(critic_state_dict)\n",
        "        # self.target_actor.load_state_dict(actor_state_dict)\n",
        "\n",
        "        #self.target_critic.load_state_dict(critic_state_dict, strict=False)\n",
        "        #self.target_actor.load_state_dict(actor_state_dict, strict=False)\n",
        "\n",
        "    def print_args(self):\n",
        "        print('alpha    : ' +str(self.alpha))\n",
        "        print('beta     : ' +str(self.beta))\n",
        "        print('gamma    : ' +str(self.gamma))\n",
        "        print('tau      : ' +str(self.tau))\n",
        "        print('noise_sig: ' +str(self.noise_sigma))\n",
        "        print('fc1_dims : ' +str(self.fc1_dims))\n",
        "        print('fc2_dims : ' +str(self.fc2_dims))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting DDPG_Agent.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCxRAai17JWa"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdzejSm81kwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b0a515-c965-4d46-850a-d6fd51c7a756"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import numpy as np\n",
        "import wandb\n",
        "from DDPG_Agent import Agent\n",
        "import argparse\n",
        "import gym \n",
        "import pprint \n",
        "\n",
        "def main():\n",
        "    wandb.init(project='AMAS_Project_DDPG', config=args)\n",
        "    pprint.pprint(args)\n",
        "    env = gym.make('LunarLanderContinuous-v2')\n",
        "    agent = Agent(alpha = args.alpha, beta = args.beta, \n",
        "                tau = args.tau, gamma = args.gamma,\n",
        "                fc1_dims = args.fc1_dims, fc2_dims=args.fc2_dims,\n",
        "                batch_size = args.batch_size, \n",
        "                n_actions=4, input_dims = env.observation_space.shape)\n",
        "    \n",
        "    agent.print_args()\n",
        "\n",
        "    best_avg_score, weighted_best_score = train(env, agent, episodes = args.episodes, avg_len = args.avg_len, burn_in_time = args.burn_in_time)\n",
        "\n",
        "    wandb.log({'best_avg_score': best_avg_score,'weighted_best_score':weighted_best_score})\n",
        "\n",
        "def train(env, agent, episodes=500, avg_len = 50, burn_in_time = 50):\n",
        "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "\n",
        "    \n",
        "    scores = []\n",
        "    best_score = float('-inf')\n",
        "\n",
        "    for i in range(episodes):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        agent.noise.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.remember(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        \n",
        "\n",
        "        avg_score = np.mean(scores[-avg_len:])\n",
        "        if avg_score > best_score and i > burn_in_time:\n",
        "          best_score = avg_score\n",
        "\n",
        "        print('episode ', i, 'score %.1f' % score,\n",
        "            'average score %.1f' % avg_score, 'best score:%.1f' %best_score)\n",
        "  \n",
        "        wandb.log({'Scores': score,'Avg_Score': avg_score, 'episodes': episodes})\n",
        "\n",
        "    weighted_best_score =np.max( np.array(scores[burn_in_time:])/np.arange(len(scores))[burn_in_time:]) \n",
        "    print(weighted_best_score)\n",
        "    return best_score, weighted_best_score\n",
        "\n",
        "def argumentParser():\n",
        "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('--alpha',        default = 2.5e-5, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--beta',         default = 0.00025, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--gamma',        default = 0.99, type=float, help='Discounting Factor')\n",
        "    parser.add_argument('--tau',          default = 0.001, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--episodes',     default = 4000, type=int, help='number of episodes')\n",
        "    parser.add_argument('--burn_in_time', default = 50, type=int, help='number of episodes before calculating avg score')\n",
        "    parser.add_argument('--avg_len',      default = 50, type=int, help='number of episodes avg is calculated over')\n",
        "    parser.add_argument('--fc1_dims',     default = 400, type=int, help='size of first fully conected layer in the network')\n",
        "    parser.add_argument('--fc2_dims',     default = 300, type=int, help='size of second fully conected layer in the network')\n",
        "    parser.add_argument('--batch_size',   default = 64, type=int, help='size of second fully conected layer in the network')\n",
        "\n",
        "    return parser\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  global args\n",
        "  args = argumentParser().parse_args()\n",
        "  main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL4A5lD1tTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a6b2c7-dffd-4e17-bb1b-e32fd6504c68"
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoustholmes\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "2021-03-26 10:41:55.798960: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdazzling-vortex-146\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/runs/ruo5bbco\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210326_104154-ruo5bbco\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Namespace(alpha=2.5e-05, avg_len=50, batch_size=64, beta=0.00025, burn_in_time=50, episodes=4000, fc1_dims=400, fc2_dims=300, gamma=0.99, tau=0.001)\n",
            "alpha    : 2.5e-05\n",
            "beta     : 0.00025\n",
            "gamma    : 0.99\n",
            "tau      : 0.001\n",
            "noise_sig: 0.15\n",
            "fc1_dims : 400\n",
            "fc2_dims : 300\n",
            "episode  0 score -275.8 average score -275.8 best score:-inf\n",
            "episode  1 score -111.2 average score -193.5 best score:-inf\n",
            "episode  2 score -407.9 average score -265.0 best score:-inf\n",
            "episode  3 score -729.9 average score -381.2 best score:-inf\n",
            "episode  4 score -435.3 average score -392.0 best score:-inf\n",
            "episode  5 score -1488.2 average score -574.7 best score:-inf\n",
            "episode  6 score -1219.6 average score -666.8 best score:-inf\n",
            "episode  7 score -817.1 average score -685.6 best score:-inf\n",
            "episode  8 score -767.6 average score -694.7 best score:-inf\n",
            "episode  9 score -717.6 average score -697.0 best score:-inf\n",
            "episode  10 score -668.3 average score -694.4 best score:-inf\n",
            "episode  11 score -690.0 average score -694.0 best score:-inf\n",
            "episode  12 score -448.7 average score -675.2 best score:-inf\n",
            "episode  13 score -642.6 average score -672.8 best score:-inf\n",
            "episode  14 score -712.6 average score -675.5 best score:-inf\n",
            "episode  15 score -812.1 average score -684.0 best score:-inf\n",
            "episode  16 score -439.4 average score -669.6 best score:-inf\n",
            "episode  17 score -370.0 average score -653.0 best score:-inf\n",
            "episode  18 score -390.5 average score -639.2 best score:-inf\n",
            "episode  19 score -310.9 average score -622.8 best score:-inf\n",
            "episode  20 score -425.6 average score -613.4 best score:-inf\n",
            "episode  21 score -14.0 average score -586.1 best score:-inf\n",
            "episode  22 score -123.9 average score -566.0 best score:-inf\n",
            "episode  23 score -103.3 average score -546.8 best score:-inf\n",
            "episode  24 score -114.3 average score -529.5 best score:-inf\n",
            "episode  25 score -168.7 average score -515.6 best score:-inf\n",
            "episode  26 score -136.2 average score -501.5 best score:-inf\n",
            "episode  27 score -280.1 average score -493.6 best score:-inf\n",
            "episode  28 score -320.0 average score -487.6 best score:-inf\n",
            "episode  29 score -349.3 average score -483.0 best score:-inf\n",
            "episode  30 score -542.6 average score -484.9 best score:-inf\n",
            "episode  31 score -558.0 average score -487.2 best score:-inf\n",
            "episode  32 score -457.3 average score -486.3 best score:-inf\n",
            "episode  33 score -515.3 average score -487.2 best score:-inf\n",
            "episode  34 score -398.8 average score -484.7 best score:-inf\n",
            "episode  35 score -393.4 average score -482.1 best score:-inf\n",
            "episode  36 score -151.0 average score -473.2 best score:-inf\n",
            "episode  37 score -165.7 average score -465.1 best score:-inf\n",
            "episode  38 score -177.8 average score -457.7 best score:-inf\n",
            "episode  39 score -325.6 average score -454.4 best score:-inf\n",
            "episode  40 score -161.6 average score -447.3 best score:-inf\n",
            "episode  41 score -283.4 average score -443.4 best score:-inf\n",
            "episode  42 score -154.8 average score -436.7 best score:-inf\n",
            "episode  43 score -174.5 average score -430.7 best score:-inf\n",
            "episode  44 score -102.1 average score -423.4 best score:-inf\n",
            "episode  45 score -455.9 average score -424.1 best score:-inf\n",
            "episode  46 score -107.8 average score -417.4 best score:-inf\n",
            "episode  47 score -147.9 average score -411.8 best score:-inf\n",
            "episode  48 score -120.8 average score -405.8 best score:-inf\n",
            "episode  49 score -119.6 average score -400.1 best score:-inf\n",
            "episode  50 score -161.2 average score -397.8 best score:-inf\n",
            "episode  51 score -132.8 average score -398.2 best score:-398.2\n",
            "episode  52 score -96.5 average score -392.0 best score:-392.0\n",
            "episode  53 score -355.0 average score -384.5 best score:-384.5\n",
            "episode  54 score -218.1 average score -380.2 best score:-380.2\n",
            "episode  55 score -628.2 average score -363.0 best score:-363.0\n",
            "episode  56 score -414.2 average score -346.9 best score:-346.9\n",
            "episode  57 score -209.3 average score -334.7 best score:-334.7\n",
            "episode  58 score 161.4 average score -316.1 best score:-316.1\n",
            "episode  59 score 218.8 average score -297.4 best score:-297.4\n",
            "episode  60 score -192.8 average score -287.9 best score:-287.9\n",
            "episode  61 score -274.4 average score -279.6 best score:-279.6\n",
            "episode  62 score -331.2 average score -277.2 best score:-277.2\n",
            "episode  63 score -302.1 average score -270.4 best score:-270.4\n",
            "episode  64 score -227.4 average score -260.7 best score:-260.7\n",
            "episode  65 score -275.4 average score -250.0 best score:-250.0\n",
            "episode  66 score -171.9 average score -244.6 best score:-244.6\n",
            "episode  67 score 5.6 average score -237.1 best score:-237.1\n",
            "episode  68 score -302.3 average score -235.3 best score:-235.3\n",
            "episode  69 score -260.8 average score -234.3 best score:-234.3\n",
            "episode  70 score -334.9 average score -232.5 best score:-232.5\n",
            "episode  71 score -450.6 average score -241.3 best score:-232.5\n",
            "episode  72 score -243.9 average score -243.7 best score:-232.5\n",
            "episode  73 score -352.2 average score -248.6 best score:-232.5\n",
            "episode  74 score -328.1 average score -252.9 best score:-232.5\n",
            "episode  75 score -279.9 average score -255.1 best score:-232.5\n",
            "episode  76 score -275.7 average score -257.9 best score:-232.5\n",
            "episode  77 score -323.6 average score -258.8 best score:-232.5\n",
            "episode  78 score -469.0 average score -261.8 best score:-232.5\n",
            "episode  79 score -562.4 average score -266.0 best score:-232.5\n",
            "episode  80 score -498.5 average score -265.2 best score:-232.5\n",
            "episode  81 score -504.8 average score -264.1 best score:-232.5\n",
            "episode  82 score -527.1 average score -265.5 best score:-232.5\n",
            "episode  83 score -345.6 average score -262.1 best score:-232.5\n",
            "episode  84 score -417.9 average score -262.5 best score:-232.5\n",
            "episode  85 score -324.2 average score -261.1 best score:-232.5\n",
            "episode  86 score -344.4 average score -265.0 best score:-232.5\n",
            "episode  87 score -330.1 average score -268.3 best score:-232.5\n",
            "episode  88 score -353.7 average score -271.8 best score:-232.5\n",
            "episode  89 score -425.4 average score -273.8 best score:-232.5\n",
            "episode  90 score -596.4 average score -282.5 best score:-232.5\n",
            "episode  91 score -593.2 average score -288.7 best score:-232.5\n",
            "episode  92 score -489.0 average score -295.3 best score:-232.5\n",
            "episode  93 score -558.6 average score -303.0 best score:-232.5\n",
            "episode  94 score -572.9 average score -312.4 best score:-232.5\n",
            "episode  95 score -552.6 average score -314.4 best score:-232.5\n",
            "episode  96 score -374.9 average score -319.7 best score:-232.5\n",
            "episode  97 score -257.0 average score -321.9 best score:-232.5\n",
            "episode  98 score -204.7 average score -323.6 best score:-232.5\n",
            "episode  99 score -242.9 average score -326.0 best score:-232.5\n",
            "episode  100 score -257.2 average score -328.0 best score:-232.5\n",
            "episode  101 score -267.0 average score -330.6 best score:-232.5\n",
            "episode  102 score -58.2 average score -329.9 best score:-232.5\n",
            "episode  103 score -69.5 average score -324.2 best score:-232.5\n",
            "episode  104 score -41.4 average score -320.6 best score:-232.5\n",
            "episode  105 score -127.0 average score -310.6 best score:-232.5\n",
            "episode  106 score -94.0 average score -304.2 best score:-232.5\n",
            "episode  107 score -106.8 average score -302.2 best score:-232.5\n",
            "episode  108 score -131.1 average score -308.0 best score:-232.5\n",
            "episode  109 score -135.0 average score -315.1 best score:-232.5\n",
            "episode  110 score -186.2 average score -315.0 best score:-232.5\n",
            "episode  111 score -188.7 average score -313.2 best score:-232.5\n",
            "episode  112 score -88.6 average score -308.4 best score:-232.5\n",
            "episode  113 score -194.9 average score -306.2 best score:-232.5\n",
            "episode  114 score -222.5 average score -306.1 best score:-232.5\n",
            "episode  115 score -269.0 average score -306.0 best score:-232.5\n",
            "episode  116 score -221.7 average score -307.0 best score:-232.5\n",
            "episode  117 score -238.0 average score -311.9 best score:-232.5\n",
            "episode  118 score -156.0 average score -309.0 best score:-232.5\n",
            "episode  119 score -83.6 average score -305.4 best score:-232.5\n",
            "episode  120 score -248.6 average score -303.7 best score:-232.5\n",
            "episode  121 score -170.0 average score -298.1 best score:-232.5\n",
            "episode  122 score -123.1 average score -295.7 best score:-232.5\n",
            "episode  123 score -166.6 average score -291.9 best score:-232.5\n",
            "episode  124 score -79.0 average score -287.0 best score:-232.5\n",
            "episode  125 score -57.3 average score -282.5 best score:-232.5\n",
            "episode  126 score -217.9 average score -281.4 best score:-232.5\n",
            "episode  127 score -90.8 average score -276.7 best score:-232.5\n",
            "episode  128 score -231.8 average score -272.0 best score:-232.5\n",
            "episode  129 score -206.2 average score -264.8 best score:-232.5\n",
            "episode  130 score -151.0 average score -257.9 best score:-232.5\n",
            "episode  131 score -122.3 average score -250.2 best score:-232.5\n",
            "episode  132 score -133.0 average score -242.4 best score:-232.5\n",
            "episode  133 score -206.3 average score -239.6 best score:-232.5\n",
            "episode  134 score -194.6 average score -235.1 best score:-232.5\n",
            "episode  135 score -207.6 average score -232.8 best score:-232.5\n",
            "episode  136 score -219.7 average score -230.3 best score:-230.3\n",
            "episode  137 score -199.5 average score -227.7 best score:-227.7\n",
            "episode  138 score -232.0 average score -225.2 best score:-225.2\n",
            "episode  139 score -72.1 average score -218.2 best score:-218.2\n",
            "episode  140 score -170.2 average score -209.6 best score:-209.6\n",
            "episode  141 score 3.5 average score -197.7 best score:-197.7\n",
            "episode  142 score -203.5 average score -192.0 best score:-192.0\n",
            "episode  143 score -242.4 average score -185.7 best score:-185.7\n",
            "episode  144 score -86.5 average score -175.9 best score:-175.9\n",
            "episode  145 score -271.1 average score -170.3 best score:-170.3\n",
            "episode  146 score -150.7 average score -165.8 best score:-165.8\n",
            "episode  147 score -283.5 average score -166.4 best score:-165.8\n",
            "episode  148 score -231.4 average score -166.9 best score:-165.8\n",
            "episode  149 score -250.3 average score -167.0 best score:-165.8\n",
            "episode  150 score -287.3 average score -167.6 best score:-165.8\n",
            "episode  151 score -231.8 average score -166.9 best score:-165.8\n",
            "episode  152 score -261.7 average score -171.0 best score:-165.8\n",
            "episode  153 score -318.2 average score -176.0 best score:-165.8\n",
            "episode  154 score -298.9 average score -181.1 best score:-165.8\n",
            "episode  155 score -260.0 average score -183.8 best score:-165.8\n",
            "episode  156 score -251.3 average score -186.9 best score:-165.8\n",
            "episode  157 score -251.1 average score -189.8 best score:-165.8\n",
            "episode  158 score -330.5 average score -193.8 best score:-165.8\n",
            "episode  159 score -272.6 average score -196.6 best score:-165.8\n",
            "episode  160 score -578.2 average score -204.4 best score:-165.8\n",
            "episode  161 score -734.4 average score -215.3 best score:-165.8\n",
            "episode  162 score -386.9 average score -221.3 best score:-165.8\n",
            "episode  163 score -312.1 average score -223.6 best score:-165.8\n",
            "episode  164 score -182.2 average score -222.8 best score:-165.8\n",
            "episode  165 score -461.2 average score -226.7 best score:-165.8\n",
            "episode  166 score -336.2 average score -229.0 best score:-165.8\n",
            "episode  167 score -405.8 average score -232.3 best score:-165.8\n",
            "episode  168 score -256.5 average score -234.3 best score:-165.8\n",
            "episode  169 score -183.4 average score -236.3 best score:-165.8\n",
            "episode  170 score -89.9 average score -233.1 best score:-165.8\n",
            "episode  171 score -104.4 average score -231.8 best score:-165.8\n",
            "episode  172 score -184.0 average score -233.0 best score:-165.8\n",
            "episode  173 score -118.3 average score -232.1 best score:-165.8\n",
            "episode  174 score -177.3 average score -234.0 best score:-165.8\n",
            "episode  175 score -73.5 average score -234.4 best score:-165.8\n",
            "episode  176 score -188.2 average score -233.8 best score:-165.8\n",
            "episode  177 score -235.5 average score -236.7 best score:-165.8\n",
            "episode  178 score -221.2 average score -236.5 best score:-165.8\n",
            "episode  179 score -217.0 average score -236.7 best score:-165.8\n",
            "episode  180 score -226.9 average score -238.2 best score:-165.8\n",
            "episode  181 score -170.5 average score -239.2 best score:-165.8\n",
            "episode  182 score 56.5 average score -235.4 best score:-165.8\n",
            "episode  183 score 37.8 average score -230.5 best score:-165.8\n",
            "episode  184 score -167.0 average score -229.9 best score:-165.8\n",
            "episode  185 score -239.1 average score -230.6 best score:-165.8\n",
            "episode  186 score -216.6 average score -230.5 best score:-165.8\n",
            "episode  187 score -213.8 average score -230.8 best score:-165.8\n",
            "episode  188 score -201.3 average score -230.2 best score:-165.8\n",
            "episode  189 score -156.9 average score -231.9 best score:-165.8\n",
            "episode  190 score -265.9 average score -233.8 best score:-165.8\n",
            "episode  191 score -235.6 average score -238.6 best score:-165.8\n",
            "episode  192 score -169.3 average score -237.9 best score:-165.8\n",
            "episode  193 score -431.7 average score -241.7 best score:-165.8\n",
            "episode  194 score -167.2 average score -243.3 best score:-165.8\n",
            "episode  195 score -195.9 average score -241.8 best score:-165.8\n",
            "episode  196 score -288.9 average score -244.5 best score:-165.8\n",
            "episode  197 score -235.6 average score -243.6 best score:-165.8\n",
            "episode  198 score -250.6 average score -244.0 best score:-165.8\n",
            "episode  199 score -202.7 average score -243.0 best score:-165.8\n",
            "episode  200 score -306.9 average score -243.4 best score:-165.8\n",
            "episode  201 score -230.0 average score -243.4 best score:-165.8\n",
            "episode  202 score -228.4 average score -242.7 best score:-165.8\n",
            "episode  203 score -381.2 average score -244.0 best score:-165.8\n",
            "episode  204 score -294.8 average score -243.9 best score:-165.8\n",
            "episode  205 score -272.6 average score -244.1 best score:-165.8\n",
            "episode  206 score -274.6 average score -244.6 best score:-165.8\n",
            "episode  207 score -337.5 average score -246.3 best score:-165.8\n",
            "episode  208 score -191.0 average score -243.5 best score:-165.8\n",
            "episode  209 score -159.6 average score -241.3 best score:-165.8\n",
            "episode  210 score 39.4 average score -228.9 best score:-165.8\n",
            "episode  211 score -72.2 average score -215.7 best score:-165.8\n",
            "episode  212 score -710.7 average score -222.2 best score:-165.8\n",
            "episode  213 score -224.0 average score -220.4 best score:-165.8\n",
            "episode  214 score -1551.5 average score -247.8 best score:-165.8\n",
            "episode  215 score -82.9 average score -240.2 best score:-165.8\n",
            "episode  216 score -86.9 average score -235.2 best score:-165.8\n",
            "episode  217 score -173.1 average score -230.6 best score:-165.8\n",
            "episode  218 score -170.3 average score -228.8 best score:-165.8\n",
            "episode  219 score -258.5 average score -230.4 best score:-165.8\n",
            "episode  220 score -217.4 average score -232.9 best score:-165.8\n",
            "episode  221 score -193.1 average score -234.7 best score:-165.8\n",
            "episode  222 score -242.9 average score -235.9 best score:-165.8\n",
            "episode  223 score -268.8 average score -238.9 best score:-165.8\n",
            "episode  224 score -199.3 average score -239.3 best score:-165.8\n",
            "episode  225 score -339.0 average score -244.6 best score:-165.8\n",
            "episode  226 score -242.3 average score -245.7 best score:-165.8\n",
            "episode  227 score -319.6 average score -247.4 best score:-165.8\n",
            "episode  228 score -169.5 average score -246.3 best score:-165.8\n",
            "episode  229 score -287.8 average score -247.8 best score:-165.8\n",
            "episode  230 score -310.6 average score -249.4 best score:-165.8\n",
            "episode  231 score -261.0 average score -251.2 best score:-165.8\n",
            "episode  232 score -244.1 average score -257.3 best score:-165.8\n",
            "episode  233 score -193.1 average score -261.9 best score:-165.8\n",
            "episode  234 score -232.1 average score -263.2 best score:-165.8\n",
            "episode  235 score -211.3 average score -262.6 best score:-165.8\n",
            "episode  236 score -82.9 average score -259.9 best score:-165.8\n",
            "episode  237 score -257.2 average score -260.8 best score:-165.8\n",
            "episode  238 score -218.0 average score -261.1 best score:-165.8\n",
            "episode  239 score -249.4 average score -263.0 best score:-165.8\n",
            "episode  240 score -189.7 average score -261.5 best score:-165.8\n",
            "episode  241 score 216.8 average score -252.4 best score:-165.8\n",
            "episode  242 score -221.5 average score -253.5 best score:-165.8\n",
            "episode  243 score -280.0 average score -250.4 best score:-165.8\n",
            "episode  244 score -254.2 average score -252.2 best score:-165.8\n",
            "episode  245 score -247.7 average score -253.2 best score:-165.8\n",
            "episode  246 score -251.8 average score -252.5 best score:-165.8\n",
            "episode  247 score -187.8 average score -251.5 best score:-165.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCgWrzN7PAd"
      },
      "source": [
        "## Sweep for Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R-TWuGmAYuH",
        "outputId": "b38a9907-3a32-472c-e887-96dfc9191e0a"
      },
      "source": [
        "%%writefile sweep.yaml\n",
        "project: \"AMAS_Project_DDPG\"\n",
        "program: main.py\n",
        "method: bayes\n",
        "metric:\n",
        "  name: best_score\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  alpha:\n",
        "    values: [0.0001, 0.0005, 0.000025, 0.00005, 0.00001, 0.000005]\n",
        "  beta:\n",
        "    values: [0.001, 0.005, 0.00025, 0.0005, 0.0001, 0.00005]\n",
        "  gamma:\n",
        "    values: [0.999, 0.99, 0.9, 0.5]\n",
        "  tau:\n",
        "    values: [ 0.008, 0.005, 0.002, 0.001, 0.0008, 0.0005, 0.0001]\n",
        "  fc1_dims:\n",
        "    values: [128, 256, 384, 512]\n",
        "  fc2_dims:\n",
        "    values: [128, 256, 384, 512]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing sweep.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX6x77OJXhWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711fa1fd-3969-4b2f-e526-707347e4a0e7"
      },
      "source": [
        "!wandb sweep sweep.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: sweep.yaml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33m1rluf6s2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/sweeps/1rluf6s2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent moustholmes/AMAS_Project_DDPG/1rluf6s2\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSFbSKvXXluL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53da0c3f-f3d9-4697-da7f-2e51ffcaa6fa"
      },
      "source": [
        "!wandb agent moustholmes/AMAS_Project_DDPG/xt7dgddd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
            "2021-03-25 10:56:10,786 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-03-25 10:56:11,075 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-25 10:56:11,075 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\talpha: 2.5e-05\n",
            "\tbeta: 0.0001\n",
            "\tfc1_dims: 256\n",
            "\tfc2_dims: 384\n",
            "\tgamma: 0.99\n",
            "\ttau: 0.0005\n",
            "2021-03-25 10:56:11,076 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --alpha=2.5e-05 --beta=0.0001 --fc1_dims=256 --fc2_dims=384 --gamma=0.99 --tau=0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaskel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-03-25 10:56:13.951101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlively-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/sweeps/xt7dgddd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/runs/qnedgbb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210325_105612-qnedgbb2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Namespace(alpha=2.5e-05, avg_len=50, batch_size=64, beta=0.0001, burn_in_time=50, episodes=2500, fc1_dims=256, fc2_dims=384, gamma=0.99, tau=0.0005)\n",
            "2021-03-25 10:56:16,087 - wandb.wandb_agent - INFO - Running runs: ['qnedgbb2']\n",
            "alpha    : 2.5e-05\n",
            "beta     : 0.0001\n",
            "gamma    : 0.99\n",
            "tau      : 0.0005\n",
            "noise_sig: 0.15\n",
            "fc1_dims : 256\n",
            "fc2_dims : 384\n",
            "episode  0 score -130.2 average score -130.2 best score:-inf\n",
            "episode  1 score -703.0 average score -416.6 best score:-inf\n",
            "episode  2 score -117.2 average score -316.8 best score:-inf\n",
            "episode  3 score -480.6 average score -357.8 best score:-inf\n",
            "episode  4 score -357.6 average score -357.7 best score:-inf\n",
            "episode  5 score -535.6 average score -387.4 best score:-inf\n",
            "episode  6 score -835.8 average score -451.4 best score:-inf\n",
            "episode  7 score -526.0 average score -460.8 best score:-inf\n",
            "episode  8 score -1104.1 average score -532.2 best score:-inf\n",
            "episode  9 score -1170.4 average score -596.1 best score:-inf\n",
            "episode  10 score -1172.5 average score -648.5 best score:-inf\n",
            "episode  11 score -925.9 average score -671.6 best score:-inf\n",
            "episode  12 score -812.5 average score -682.4 best score:-inf\n",
            "episode  13 score -1131.0 average score -714.5 best score:-inf\n",
            "episode  14 score -1449.1 average score -763.4 best score:-inf\n",
            "episode  15 score -1447.2 average score -806.2 best score:-inf\n",
            "episode  16 score -364.9 average score -780.2 best score:-inf\n",
            "episode  17 score -480.2 average score -763.6 best score:-inf\n",
            "episode  18 score -459.0 average score -747.5 best score:-inf\n",
            "episode  19 score -612.5 average score -740.8 best score:-inf\n",
            "episode  20 score -582.9 average score -733.2 best score:-inf\n",
            "episode  21 score -78.6 average score -703.5 best score:-inf\n",
            "episode  22 score -304.7 average score -686.2 best score:-inf\n",
            "episode  23 score -652.1 average score -684.7 best score:-inf\n",
            "episode  24 score -716.1 average score -686.0 best score:-inf\n",
            "episode  25 score -764.5 average score -689.0 best score:-inf\n",
            "episode  26 score -712.2 average score -689.9 best score:-inf\n",
            "episode  27 score -506.9 average score -683.3 best score:-inf\n",
            "episode  28 score -846.0 average score -688.9 best score:-inf\n",
            "episode  29 score -935.4 average score -697.2 best score:-inf\n",
            "episode  30 score -452.1 average score -689.3 best score:-inf\n",
            "episode  31 score -663.7 average score -688.5 best score:-inf\n",
            "episode  32 score -360.2 average score -678.5 best score:-inf\n",
            "episode  33 score -673.1 average score -678.3 best score:-inf\n",
            "episode  34 score -738.8 average score -680.1 best score:-inf\n",
            "episode  35 score -631.1 average score -678.7 best score:-inf\n",
            "episode  36 score -587.3 average score -676.2 best score:-inf\n",
            "episode  37 score -705.3 average score -677.0 best score:-inf\n",
            "episode  38 score -703.7 average score -677.7 best score:-inf\n",
            "episode  39 score -604.7 average score -675.9 best score:-inf\n",
            "episode  40 score -256.1 average score -665.6 best score:-inf\n",
            "episode  41 score -165.2 average score -653.7 best score:-inf\n",
            "episode  42 score -98.9 average score -640.8 best score:-inf\n",
            "episode  43 score -144.0 average score -629.5 best score:-inf\n",
            "episode  44 score -126.7 average score -618.4 best score:-inf\n",
            "episode  45 score -119.3 average score -607.5 best score:-inf\n",
            "episode  46 score -195.3 average score -598.7 best score:-inf\n",
            "episode  47 score -139.8 average score -589.2 best score:-inf\n",
            "episode  48 score -373.3 average score -584.8 best score:-inf\n",
            "episode  49 score -358.2 average score -580.2 best score:-inf\n",
            "episode  50 score -482.7 average score -587.3 best score:-inf\n",
            "episode  51 score -613.8 average score -585.5 best score:-585.5\n",
            "episode  52 score -139.0 average score -585.9 best score:-585.5\n",
            "episode  53 score -480.1 average score -585.9 best score:-585.5\n",
            "episode  54 score -598.3 average score -590.7 best score:-585.5\n",
            "episode  55 score -505.6 average score -590.1 best score:-585.5\n",
            "episode  56 score -571.4 average score -584.8 best score:-584.8\n",
            "episode  57 score -550.2 average score -585.3 best score:-584.8\n",
            "episode  58 score -502.4 average score -573.3 best score:-573.3\n",
            "episode  59 score -571.1 average score -561.3 best score:-561.3\n",
            "episode  60 score -568.3 average score -549.2 best score:-549.2\n",
            "episode  61 score -684.4 average score -544.4 best score:-544.4\n",
            "episode  62 score -627.0 average score -540.7 best score:-540.7\n",
            "episode  63 score -528.2 average score -528.6 best score:-528.6\n",
            "episode  64 score -762.8 average score -514.9 best score:-514.9\n",
            "episode  65 score -701.1 average score -500.0 best score:-500.0\n",
            "episode  66 score -643.7 average score -505.6 best score:-500.0\n",
            "episode  67 score -634.2 average score -508.6 best score:-500.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcpxJMqh6qzH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}