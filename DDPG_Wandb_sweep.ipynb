{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG_Wandb_sweep.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoustHolmes/AMAS_Project/blob/main/DDPG_Wandb_sweep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oQTNVeq63Cm"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgdLryIc12tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f20f466-9dd7-49fe-8b7a-97bc54cdf9e3"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip install wandb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\r\u001b[K     |‚ñä                               | 10kB 16.7MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñå                              | 20kB 9.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñè                             | 30kB 7.5MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà                             | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñã                            | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñç                           | 61kB 4.9MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                          | 71kB 5.0MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                          | 81kB 5.3MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                         | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                       | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                      | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                     | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                  | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                  | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 245kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé             | 256kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà             | 266kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä            | 276kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 286kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè          | 296kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 307kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 317kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç        | 327kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà        | 337kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ       | 348kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã      | 358kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 368kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     | 378kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 389kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 399kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 409kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 419kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 430kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 440kB 5.4MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 450kB 5.4MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/64/29b77da604e81607e35479bda8c31aabe8911c284fdad488a9030fa4cc0a/wandb-0.10.23-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.0MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 163kB 26.6MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133kB 23.1MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 102kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=6edb17258a79d99f7a9ab2039bfc7b7717fc20d315a1236505e9d5aa77e5a31c\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=192c9a76a558edf60b9543cf4f2a2a670ba191af85b41b206a72d4dd3c9f28f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: shortuuid, smmap, gitdb, GitPython, sentry-sdk, subprocess32, docker-pycreds, configparser, pathtools, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL8C74KZ9GqA",
        "outputId": "f3cdb659-f9bc-44d8-d8bb-c1f706c8b4b6"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXeDEL7e7D8o"
      },
      "source": [
        "## Define the Q Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOpZru0S1fOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25d3016a-14d7-4c6b-96c6-02bf889423db"
      },
      "source": [
        "%%writefile DDPG_Agent.py\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                 chkpt_dir='/content/gdrive/My Drive'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.name = name\n",
        "        model_save_name = name + '_2.pt'\n",
        "        # self.path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        self.checkpoint_dir = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "        # self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "\n",
        "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
        "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
        "\n",
        "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
        "        \n",
        "        self.q = nn.Linear(self.fc2_dims, 1)\n",
        "\n",
        "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        self.fc1.weight.data.uniform_(-f1, f1)\n",
        "        self.fc1.bias.data.uniform_(-f1, f1)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        self.fc2.weight.data.uniform_(-f2, f2)\n",
        "        self.fc2.bias.data.uniform_(-f2, f2)\n",
        "\n",
        "        f3 = 0.003\n",
        "        self.q.weight.data.uniform_(-f3, f3)\n",
        "        self.q.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "        f4 = 1./np.sqrt(self.action_value.weight.data.size()[0])\n",
        "        \n",
        "        self.action_value.weight.data.uniform_(-f4, f4)\n",
        "        self.action_value.bias.data.uniform_(-f4, f4)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta,\n",
        "                                    weight_decay=0.01)\n",
        "        \n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_value = self.fc1(state)\n",
        "        state_value = self.bn1(state_value)\n",
        "        state_value = F.relu(state_value)\n",
        "        state_value = self.fc2(state_value)\n",
        "        state_value = self.bn2(state_value)\n",
        "        #state_value = F.relu(state_value)\n",
        "        #action_value = F.relu(self.action_value(action))\n",
        "        action_value = self.action_value(action)\n",
        "        state_action_value = F.relu(T.add(state_value, action_value))\n",
        "        #state_action_value = T.add(state_value, action_value)\n",
        "        state_action_value = self.q(state_action_value)\n",
        "\n",
        "        return state_action_value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        # T.save(self.state_dict(), self.path)\n",
        "        # T.save(self.state_dict(), self.checkpoint_file)\n",
        "        T.save(self.state_dict(), self.checkpoint_dir)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_dir))\n",
        "\n",
        "    def save_best(self):\n",
        "        print('... saving best checkpoint ...')\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
        "        T.save(self.state_dict(), checkpoint_file)\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
        "                 chkpt_dir='/content/gdrive/My Drive'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.name = name\n",
        "        model_save_name = name + '_2.pt'\n",
        "        # self.path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "        self.checkpoint_dir = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "        # self.checkpoint_dir = chkpt_dir\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "\n",
        "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
        "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
        "\n",
        "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
        "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
        "\n",
        "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
        "\n",
        "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        self.fc2.weight.data.uniform_(-f2, f2)\n",
        "        self.fc2.bias.data.uniform_(-f2, f2)\n",
        "\n",
        "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        self.fc1.weight.data.uniform_(-f1, f1)\n",
        "        self.fc1.bias.data.uniform_(-f1, f1)\n",
        "\n",
        "        f3 = 0.003\n",
        "        self.mu.weight.data.uniform_(-f3, f3)\n",
        "        self.mu.bias.data.uniform_(-f3, f3)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = T.tanh(self.mu(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        print('... saving checkpoint ...')\n",
        "        # T.save(self.state_dict(), self.checkpoint_file)\n",
        "        T.save(self.state_dict(), self.checkpoint_dir)#changed file to dir \n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        print('... loading checkpoint ...')\n",
        "        self.load_state_dict(T.load(self.checkpoint_dir))#changed file to dir\n",
        "\n",
        "    def save_best(self):\n",
        "        print('... saving best checkpoint ...')\n",
        "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
        "        T.save(self.state_dict(), checkpoint_file)\n",
        "\n",
        "\n",
        "class OUActionNoise():\n",
        "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
        "        self.theta = theta\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n",
        "\n",
        "class ReplayBuffer():\n",
        "    def __init__(self, max_size, input_shape, n_actions):\n",
        "        self.mem_size = max_size\n",
        "        self.mem_cntr = 0\n",
        "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
        "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
        "        self.reward_memory = np.zeros(self.mem_size)\n",
        "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
        "\n",
        "    def store_transition(self, state, action, reward, state_, done):\n",
        "        index = self.mem_cntr % self.mem_size\n",
        "        self.state_memory[index] = state\n",
        "        self.action_memory[index] = action\n",
        "        self.reward_memory[index] = reward\n",
        "        self.new_state_memory[index] = state_\n",
        "        self.terminal_memory[index] = done\n",
        "\n",
        "        self.mem_cntr += 1\n",
        "\n",
        "    def sample_buffer(self, batch_size):\n",
        "        max_mem = min(self.mem_cntr, self.mem_size)\n",
        "\n",
        "        batch = np.random.choice(max_mem, batch_size)\n",
        "\n",
        "        states = self.state_memory[batch]\n",
        "        actions = self.action_memory[batch]\n",
        "        rewards = self.reward_memory[batch]\n",
        "        states_ = self.new_state_memory[batch]\n",
        "        dones = self.terminal_memory[batch]\n",
        "\n",
        "        return states, actions, rewards, states_, dones\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, input_dims, n_actions,\n",
        "                 alpha, beta, tau, gamma=0.99,\n",
        "                 output_bounds=1,\n",
        "                 fc1_dims=400, fc2_dims=300,\n",
        "                 noise_sigma = 0.15,\n",
        "                 batch_size=256,\n",
        "                 max_size=1000000):\n",
        "      #output bounds are my addition \n",
        "        self.output_bounds = output_bounds\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.tau = tau\n",
        "        self.gamma = gamma\n",
        "        self.noise_sigma = noise_sigma\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
        "        #we have  to scale the noise to match the outpu bounds\n",
        "\n",
        "        var_names = str(alpha) +'_'+ str(beta) +'_'+ str(gamma) +'_' +str(fc1_dims) +'_'+ str(fc2_dims) +'_'+ str(noise_sigma)\n",
        "\n",
        "        self.noise = OUActionNoise(mu=np.zeros(n_actions), sigma = noise_sigma *output_bounds)\n",
        "\n",
        "        self.actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions=n_actions, name='actor' +var_names)\n",
        "        self.critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions=n_actions, name='critic' +var_names)\n",
        "\n",
        "        self.target_actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions = n_actions, name = 'target_actor' +var_names)\n",
        "\n",
        "        self.target_critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
        "                                n_actions = n_actions, name = 'target_critic' +var_names)\n",
        "\n",
        "        self.update_network_parameters(tau=1)\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        self.actor.eval()\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state).to(self.actor.device)\n",
        "        mu_prime = mu + T.tensor(self.noise(), \n",
        "                                    dtype=T.float).to(self.actor.device)\n",
        "        self.actor.train()\n",
        "\n",
        "        return mu_prime.cpu().detach().numpy()[0]*self.output_bounds\n",
        "\n",
        "    def choose_action_no_noise(self, observation):#my addition\n",
        "        self.actor.eval()\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state).to(self.actor.device)\n",
        "        return mu.cpu().detach().numpy()[0]*self.output_bounds\n",
        "\n",
        "    def choose_action_no_noise_batch(self, observation):#my addition\n",
        "        self.actor.eval()\n",
        "        state = T.tensor(observation, dtype=T.float).to(self.actor.device)\n",
        "        mu = self.actor.forward(state)#.to(self.actor.device)\n",
        "        return mu.cpu().detach().numpy()*self.output_bounds\n",
        "\n",
        "    def remember(self, state, action, reward, state_, done):\n",
        "        self.memory.store_transition(state, action, reward, state_, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        self.actor.save_checkpoint()\n",
        "        self.target_actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "        self.target_critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        self.actor.load_checkpoint()\n",
        "        self.target_actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "        self.target_critic.load_checkpoint()\n",
        "\n",
        "    def learn(self):\n",
        "        if self.memory.mem_cntr < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, states_, done = \\\n",
        "                self.memory.sample_buffer(self.batch_size)\n",
        "\n",
        "        states = T.tensor(states, dtype=T.float).to(self.actor.device)\n",
        "        states_ = T.tensor(states_, dtype=T.float).to(self.actor.device)\n",
        "        actions = T.tensor(actions, dtype=T.float).to(self.actor.device)\n",
        "        rewards = T.tensor(rewards, dtype=T.float).to(self.actor.device)\n",
        "        done = T.tensor(done).to(self.actor.device)\n",
        "\n",
        "        target_actions = self.target_actor.forward(states_)\n",
        "        critic_value_ = self.target_critic.forward(states_, target_actions)\n",
        "        critic_value = self.critic.forward(states, actions)\n",
        "\n",
        "        critic_value_[done] = 0.0\n",
        "        critic_value_ = critic_value_.view(-1)\n",
        "\n",
        "        target = rewards + self.gamma*critic_value_\n",
        "        target = target.view(self.batch_size, 1)\n",
        "\n",
        "        self.critic.optimizer.zero_grad()\n",
        "        critic_loss = F.mse_loss(target, critic_value)\n",
        "        critic_loss.backward()\n",
        "        self.critic.optimizer.step()\n",
        "\n",
        "        self.actor.optimizer.zero_grad()\n",
        "        actor_loss = -self.critic.forward(states, self.actor.forward(states))\n",
        "        actor_loss = T.mean(actor_loss)\n",
        "        actor_loss.backward()\n",
        "        self.actor.optimizer.step()\n",
        "\n",
        "        self.update_network_parameters()\n",
        "\n",
        "    def update_network_parameters(self, tau=None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        actor_params = self.actor.named_parameters()\n",
        "        critic_params = self.critic.named_parameters()\n",
        "        target_actor_params = self.target_actor.named_parameters()\n",
        "        target_critic_params = self.target_critic.named_parameters()\n",
        "\n",
        "        critic_state_dict = dict(critic_params)\n",
        "        actor_state_dict = dict(actor_params)\n",
        "        target_critic_state_dict = dict(target_critic_params)\n",
        "        target_actor_state_dict = dict(target_actor_params)\n",
        "\n",
        "        for name in critic_state_dict:\n",
        "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
        "                                (1-tau)*target_critic_state_dict[name].clone()\n",
        "\n",
        "        for name in actor_state_dict:\n",
        "             actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
        "                                 (1-tau)*target_actor_state_dict[name].clone()\n",
        "\n",
        "        # self.target_critic.load_state_dict(critic_state_dict)\n",
        "        # self.target_actor.load_state_dict(actor_state_dict)\n",
        "\n",
        "        #self.target_critic.load_state_dict(critic_state_dict, strict=False)\n",
        "        #self.target_actor.load_state_dict(actor_state_dict, strict=False)\n",
        "\n",
        "    def print_args(self):\n",
        "        print('alpha    : ' +str(self.alpha))\n",
        "        print('beta     : ' +str(self.beta))\n",
        "        print('gamma    : ' +str(self.gamma))\n",
        "        print('tau      : ' +str(self.tau))\n",
        "        print('noise_sig: ' +str(self.noise_sigma))\n",
        "        print('fc1_dims : ' +str(self.fc1_dims))\n",
        "        print('fc2_dims : ' +str(self.fc2_dims))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting DDPG_Agent.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCxRAai17JWa"
      },
      "source": [
        "## Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdzejSm81kwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34fbd8b4-9bc8-4577-b935-1ebd85d84a74"
      },
      "source": [
        "%%writefile main.py\n",
        "\n",
        "import numpy as np\n",
        "import wandb\n",
        "from DDPG_Agent import Agent\n",
        "import argparse\n",
        "import gym \n",
        "import pprint \n",
        "\n",
        "def main():\n",
        "    wandb.init(project='AMAS_Project_DDPG', config=args)\n",
        "    pprint.pprint(args)\n",
        "    env = gym.make('LunarLanderContinuous-v2')\n",
        "    agent = Agent(alpha = args.alpha, beta = args.beta, \n",
        "                tau = args.tau, gamma = args.gamma,\n",
        "                fc1_dims = args.fc1_dims, fc2_dims=args.fc2_dims,\n",
        "                batch_size = args.batch_size, \n",
        "                n_actions=4, input_dims = env.observation_space.shape)\n",
        "    \n",
        "    agent.print_args()\n",
        "\n",
        "    best_avg_score, weighted_best_score = train(env, agent, episodes = args.episodes, avg_len = args.avg_len, burn_in_time = args.burn_in_time)\n",
        "\n",
        "    wandb.log({'best_avg_score': best_avg_score,'weighted_best_score':weighted_best_score})\n",
        "\n",
        "def train(env, agent, episodes=500, avg_len = 50, burn_in_time = 50):\n",
        "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
        "\n",
        "    \n",
        "    scores = []\n",
        "    best_score = float('-inf')\n",
        "\n",
        "    for i in range(episodes):\n",
        "        score = 0\n",
        "        done = False\n",
        "        observation = env.reset()\n",
        "        while not done:\n",
        "            action = agent.choose_action(observation)\n",
        "            observation_, reward, done, info = env.step(action)\n",
        "            score += reward\n",
        "            agent.remember(observation, action, reward, \n",
        "                                    observation_, done)\n",
        "            agent.learn()\n",
        "            observation = observation_\n",
        "        scores.append(score)\n",
        "        \n",
        "\n",
        "        avg_score = np.mean(scores[-avg_len:])\n",
        "        if avg_score > best_score and i > burn_in_time:\n",
        "          best_score = avg_score\n",
        "\n",
        "        print('episode ', i, 'score %.1f' % score,\n",
        "            'average score %.1f' % avg_score, 'best score:%.1f' %best_score)\n",
        "  \n",
        "        wandb.log({'Scores': score,'Avg_Score': avg_score, 'episodes': episodes})\n",
        "\n",
        "    weighted_best_score =np.max( np.array(scores[burn_in_time:])/np.arange(len(scores))[burn_in_time:]) \n",
        "    print(weighted_best_score)\n",
        "    return best_score, weighted_best_score\n",
        "\n",
        "def argumentParser():\n",
        "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('--alpha',        default = 0.000025, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--beta',         default = 0.00025, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--gamma',        default = 0.99, type=float, help='Discounting Factor')\n",
        "    parser.add_argument('--tau',          default = 0.001, type=float, help='Learning Rate')\n",
        "    parser.add_argument('--episodes',     default = 2500, type=int, help='number of episodes')\n",
        "    parser.add_argument('--burn_in_time', default = 50, type=int, help='number of episodes before calculating avg score')\n",
        "    parser.add_argument('--avg_len',      default = 50, type=int, help='number of episodes avg is calculated over')\n",
        "    parser.add_argument('--fc1_dims',     default = 400, type=int, help='size of first fully conected layer in the network')\n",
        "    parser.add_argument('--fc2_dims',     default = 300, type=int, help='size of second fully conected layer in the network')\n",
        "    parser.add_argument('--batch_size',   default = 64, type=int, help='size of second fully conected layer in the network')\n",
        "\n",
        "    return parser\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  global args\n",
        "  args = argumentParser().parse_args()\n",
        "  main()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting main.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL4A5lD1tTS"
      },
      "source": [
        "!python3 main.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plCgWrzN7PAd"
      },
      "source": [
        "## Sweep for Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R-TWuGmAYuH",
        "outputId": "25f1343f-182b-40ad-e747-c29a009c586c"
      },
      "source": [
        "%%writefile sweep.yaml\n",
        "project: \"AMAS_Project_DDPG\"\n",
        "program: main.py\n",
        "method: bayes\n",
        "metric:\n",
        "  name: best_score\n",
        "  goal: maximize\n",
        "parameters:\n",
        "  alpha:\n",
        "    values: [0.0001, 0.0005, 0.000025, 0.00005, 0.00001, 0.000005]\n",
        "  beta:\n",
        "    values: [0.001, 0.005, 0.00025, 0.0005, 0.0001, 0.00005]\n",
        "  gamma:\n",
        "    values: [0.999, 0.99, 0.9, 0.5]\n",
        "  tau:\n",
        "    values: [ 0.008, 0.005, 0.002, 0.001, 0.0008, 0.0005, 0.0001]\n",
        "  fc1_dims:\n",
        "    values: [128, 256, 384, 512]\n",
        "  fc2_dims:\n",
        "    values: [128, 256, 384, 512]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting sweep.yaml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX6x77OJXhWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711fa1fd-3969-4b2f-e526-707347e4a0e7"
      },
      "source": [
        "!wandb sweep sweep.yaml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Creating sweep from: sweep.yaml\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Created sweep with ID: \u001b[33m1rluf6s2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: View sweep at: \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/sweeps/1rluf6s2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run sweep agent with: \u001b[33mwandb agent moustholmes/AMAS_Project_DDPG/1rluf6s2\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSFbSKvXXluL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53da0c3f-f3d9-4697-da7f-2e51ffcaa6fa"
      },
      "source": [
        "!wandb agent moustholmes/AMAS_Project_DDPG/xt7dgddd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
            "2021-03-25 10:56:10,786 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-03-25 10:56:11,075 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-03-25 10:56:11,075 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\talpha: 2.5e-05\n",
            "\tbeta: 0.0001\n",
            "\tfc1_dims: 256\n",
            "\tfc2_dims: 384\n",
            "\tgamma: 0.99\n",
            "\ttau: 0.0005\n",
            "2021-03-25 10:56:11,076 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python main.py --alpha=2.5e-05 --beta=0.0001 --fc1_dims=256 --fc2_dims=384 --gamma=0.99 --tau=0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaskel\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-03-25 10:56:13.951101: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.23\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mlively-sweep-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/sweeps/xt7dgddd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/moustholmes/AMAS_Project_DDPG/runs/qnedgbb2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210325_105612-qnedgbb2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Namespace(alpha=2.5e-05, avg_len=50, batch_size=64, beta=0.0001, burn_in_time=50, episodes=2500, fc1_dims=256, fc2_dims=384, gamma=0.99, tau=0.0005)\n",
            "2021-03-25 10:56:16,087 - wandb.wandb_agent - INFO - Running runs: ['qnedgbb2']\n",
            "alpha    : 2.5e-05\n",
            "beta     : 0.0001\n",
            "gamma    : 0.99\n",
            "tau      : 0.0005\n",
            "noise_sig: 0.15\n",
            "fc1_dims : 256\n",
            "fc2_dims : 384\n",
            "episode  0 score -130.2 average score -130.2 best score:-inf\n",
            "episode  1 score -703.0 average score -416.6 best score:-inf\n",
            "episode  2 score -117.2 average score -316.8 best score:-inf\n",
            "episode  3 score -480.6 average score -357.8 best score:-inf\n",
            "episode  4 score -357.6 average score -357.7 best score:-inf\n",
            "episode  5 score -535.6 average score -387.4 best score:-inf\n",
            "episode  6 score -835.8 average score -451.4 best score:-inf\n",
            "episode  7 score -526.0 average score -460.8 best score:-inf\n",
            "episode  8 score -1104.1 average score -532.2 best score:-inf\n",
            "episode  9 score -1170.4 average score -596.1 best score:-inf\n",
            "episode  10 score -1172.5 average score -648.5 best score:-inf\n",
            "episode  11 score -925.9 average score -671.6 best score:-inf\n",
            "episode  12 score -812.5 average score -682.4 best score:-inf\n",
            "episode  13 score -1131.0 average score -714.5 best score:-inf\n",
            "episode  14 score -1449.1 average score -763.4 best score:-inf\n",
            "episode  15 score -1447.2 average score -806.2 best score:-inf\n",
            "episode  16 score -364.9 average score -780.2 best score:-inf\n",
            "episode  17 score -480.2 average score -763.6 best score:-inf\n",
            "episode  18 score -459.0 average score -747.5 best score:-inf\n",
            "episode  19 score -612.5 average score -740.8 best score:-inf\n",
            "episode  20 score -582.9 average score -733.2 best score:-inf\n",
            "episode  21 score -78.6 average score -703.5 best score:-inf\n",
            "episode  22 score -304.7 average score -686.2 best score:-inf\n",
            "episode  23 score -652.1 average score -684.7 best score:-inf\n",
            "episode  24 score -716.1 average score -686.0 best score:-inf\n",
            "episode  25 score -764.5 average score -689.0 best score:-inf\n",
            "episode  26 score -712.2 average score -689.9 best score:-inf\n",
            "episode  27 score -506.9 average score -683.3 best score:-inf\n",
            "episode  28 score -846.0 average score -688.9 best score:-inf\n",
            "episode  29 score -935.4 average score -697.2 best score:-inf\n",
            "episode  30 score -452.1 average score -689.3 best score:-inf\n",
            "episode  31 score -663.7 average score -688.5 best score:-inf\n",
            "episode  32 score -360.2 average score -678.5 best score:-inf\n",
            "episode  33 score -673.1 average score -678.3 best score:-inf\n",
            "episode  34 score -738.8 average score -680.1 best score:-inf\n",
            "episode  35 score -631.1 average score -678.7 best score:-inf\n",
            "episode  36 score -587.3 average score -676.2 best score:-inf\n",
            "episode  37 score -705.3 average score -677.0 best score:-inf\n",
            "episode  38 score -703.7 average score -677.7 best score:-inf\n",
            "episode  39 score -604.7 average score -675.9 best score:-inf\n",
            "episode  40 score -256.1 average score -665.6 best score:-inf\n",
            "episode  41 score -165.2 average score -653.7 best score:-inf\n",
            "episode  42 score -98.9 average score -640.8 best score:-inf\n",
            "episode  43 score -144.0 average score -629.5 best score:-inf\n",
            "episode  44 score -126.7 average score -618.4 best score:-inf\n",
            "episode  45 score -119.3 average score -607.5 best score:-inf\n",
            "episode  46 score -195.3 average score -598.7 best score:-inf\n",
            "episode  47 score -139.8 average score -589.2 best score:-inf\n",
            "episode  48 score -373.3 average score -584.8 best score:-inf\n",
            "episode  49 score -358.2 average score -580.2 best score:-inf\n",
            "episode  50 score -482.7 average score -587.3 best score:-inf\n",
            "episode  51 score -613.8 average score -585.5 best score:-585.5\n",
            "episode  52 score -139.0 average score -585.9 best score:-585.5\n",
            "episode  53 score -480.1 average score -585.9 best score:-585.5\n",
            "episode  54 score -598.3 average score -590.7 best score:-585.5\n",
            "episode  55 score -505.6 average score -590.1 best score:-585.5\n",
            "episode  56 score -571.4 average score -584.8 best score:-584.8\n",
            "episode  57 score -550.2 average score -585.3 best score:-584.8\n",
            "episode  58 score -502.4 average score -573.3 best score:-573.3\n",
            "episode  59 score -571.1 average score -561.3 best score:-561.3\n",
            "episode  60 score -568.3 average score -549.2 best score:-549.2\n",
            "episode  61 score -684.4 average score -544.4 best score:-544.4\n",
            "episode  62 score -627.0 average score -540.7 best score:-540.7\n",
            "episode  63 score -528.2 average score -528.6 best score:-528.6\n",
            "episode  64 score -762.8 average score -514.9 best score:-514.9\n",
            "episode  65 score -701.1 average score -500.0 best score:-500.0\n",
            "episode  66 score -643.7 average score -505.6 best score:-500.0\n",
            "episode  67 score -634.2 average score -508.6 best score:-500.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcpxJMqh6qzH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}